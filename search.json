[
  {
    "objectID": "exercises/conflicted.html",
    "href": "exercises/conflicted.html",
    "title": "{conflicted}",
    "section": "",
    "text": "The {conflicted} package provides an alternative to R’s default management of conflicts - to just use the most recently loaded package. {conflicted} instead highlights the conflict when it occurs as an error, forcing the user to be explicit about which function they would prefer to use. It also provides a function to “scout” potential conflicts, to make decisions about preference before conflicts are hit.\n\n\nAt its simplest, using {conflicted} is just loading it with library().\nlibrary(conflicted)\nA natural place for us to do this is in our R/packages.R script.\nConflicts are detected when a function is used that is defined in multiple packages. See this example from the {conflicted} documentation:\nlibrary(dplyr)\n\nfilter(mtcars, cyl == 8)\n#&gt; Error:\n#&gt; ! [conflicted] filter found in 2 packages.\n#&gt; Either pick the one you want with `::`:\n#&gt; • dplyr::filter\n#&gt; • stats::filter\n#&gt; Or declare a preference with `conflicts_prefer()`:\n#&gt; • `conflicts_prefer(dplyr::filter)`\n#&gt; • `conflicts_prefer(stats::filter)`\nAs suggested, use the conflicts_prefer() function to explicitly select which package to prefer.\nIf you’d like to look for other conflicts that may come up, use conflict_scout(). Note that you only need to prefer a package if the conflicting function is used, so it isn’t necessary to go through the whole list returned by conflict_scout() to pick each preferred function. Look for ones you know you will use and be explicit for those.\nLastly, if you want to always prefer a certain package’s functions over another package you can use conflict_prefer_all() or conflict_prefer_matching(). See more here: ?conflict_prefer.\n\n\nInstruction: open your R/packages.R script and load the {conflicted} package. In your console, run conflict_scout(). Test one of the displayed functions in the console to see what happens when you try running it, eg. filter().\n\n\n\nInstruction: now run the {targets} pipeline and see if any conflicts come up. If they do, use conflict_prefer(package::function) to specify which package you prefer."
  },
  {
    "objectID": "exercises/conflicted.html#usage",
    "href": "exercises/conflicted.html#usage",
    "title": "{conflicted}",
    "section": "",
    "text": "At its simplest, using {conflicted} is just loading it with library().\nlibrary(conflicted)\nA natural place for us to do this is in our R/packages.R script.\nConflicts are detected when a function is used that is defined in multiple packages. See this example from the {conflicted} documentation:\nlibrary(dplyr)\n\nfilter(mtcars, cyl == 8)\n#&gt; Error:\n#&gt; ! [conflicted] filter found in 2 packages.\n#&gt; Either pick the one you want with `::`:\n#&gt; • dplyr::filter\n#&gt; • stats::filter\n#&gt; Or declare a preference with `conflicts_prefer()`:\n#&gt; • `conflicts_prefer(dplyr::filter)`\n#&gt; • `conflicts_prefer(stats::filter)`\nAs suggested, use the conflicts_prefer() function to explicitly select which package to prefer.\nIf you’d like to look for other conflicts that may come up, use conflict_scout(). Note that you only need to prefer a package if the conflicting function is used, so it isn’t necessary to go through the whole list returned by conflict_scout() to pick each preferred function. Look for ones you know you will use and be explicit for those.\nLastly, if you want to always prefer a certain package’s functions over another package you can use conflict_prefer_all() or conflict_prefer_matching(). See more here: ?conflict_prefer.\n\n\nInstruction: open your R/packages.R script and load the {conflicted} package. In your console, run conflict_scout(). Test one of the displayed functions in the console to see what happens when you try running it, eg. filter().\n\n\n\nInstruction: now run the {targets} pipeline and see if any conflicts come up. If they do, use conflict_prefer(package::function) to specify which package you prefer."
  },
  {
    "objectID": "exercises/functions/01-functions-introduction.html",
    "href": "exercises/functions/01-functions-introduction.html",
    "title": "Introduction to functions",
    "section": "",
    "text": "Learning goal: Identify the structure of functions in terms of inputs, arguments, return objects and naming.\nInstruction: Pick three functions of your choice and note their characteristics. (pairs)\nFor example:\n\nname\narguments (inputs, options)\nreturn object (outputs)\n\nFeel free to look up documentation online, in the R help manual on your computer through RStudio or using the ? and ?? commands.\nWe encourage you to find your own functions, maybe a function you use or have wanted to learn about. If you can’t think of any right now, see for example stats::lm(), stats::runif(), or (trickier) base::do.call().\n\nInstruction: For the following tasks, write a function’s name, arguments and return object. Do this on paper, or in a blank text document - we are not writing code yet.\nFirst round (solo)\n\nCount the number of values greater than 0 in one column of a data.frame\nFilter rows in one column of a data.frame that match a string\nPlot a histogram for each vector of numbers in a list\n\nLearning goal: Reframe how we might think of code for an analysis as a series of steps instead as inputs, outputs, and arguments of functions.\nSecond round (together)\n\nAggregate columns of a data.frame by a grouping column, calculate a metric, and make two summary plots\nPrint summary statistics, make a diagnostic plot, filter based on an input argument, and write out a spreadsheet\n\nLearning goal: identify when a function’s goal is too complex and could be split into subfunctions that are easier to develop and test.\n\nInstruction: Read the following functions and note what you expect they will output. We have purposely used vague function names in this case - not our typical recommendation! When you are ready, click “Show output”.\nFunction 1\n\napple &lt;- 'green'\n\nf &lt;- function(x) {\n  nchar(x)\n}\n\nf(apple)\n\n\nShow output\n\n\n[1] 5\n\n\nFunction 2\n\napple &lt;- 'gala'\n\nf &lt;- function(x, times) {\n  rep(x, times = times)\n}\n\nf(apple, 10)\n\n\nShow output\n\n\n [1] \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\"\n\n\nFunction 3\n\napple &lt;- 'mcintosh'\npotato &lt;- 'russet'\n\nf &lt;- function(apple, potato) {\n  data.frame(apples = apple, potatoes = potato)\n}\n\nf(potato = potato)\n\n\nShow output\n\n\nError in f(potato = potato): argument \"apple\" is missing, with no default"
  },
  {
    "objectID": "exercises/functions/01-functions-introduction.html#exercises-function-introduction",
    "href": "exercises/functions/01-functions-introduction.html#exercises-function-introduction",
    "title": "Introduction to functions",
    "section": "",
    "text": "Learning goal: Identify the structure of functions in terms of inputs, arguments, return objects and naming.\nInstruction: Pick three functions of your choice and note their characteristics. (pairs)\nFor example:\n\nname\narguments (inputs, options)\nreturn object (outputs)\n\nFeel free to look up documentation online, in the R help manual on your computer through RStudio or using the ? and ?? commands.\nWe encourage you to find your own functions, maybe a function you use or have wanted to learn about. If you can’t think of any right now, see for example stats::lm(), stats::runif(), or (trickier) base::do.call().\n\nInstruction: For the following tasks, write a function’s name, arguments and return object. Do this on paper, or in a blank text document - we are not writing code yet.\nFirst round (solo)\n\nCount the number of values greater than 0 in one column of a data.frame\nFilter rows in one column of a data.frame that match a string\nPlot a histogram for each vector of numbers in a list\n\nLearning goal: Reframe how we might think of code for an analysis as a series of steps instead as inputs, outputs, and arguments of functions.\nSecond round (together)\n\nAggregate columns of a data.frame by a grouping column, calculate a metric, and make two summary plots\nPrint summary statistics, make a diagnostic plot, filter based on an input argument, and write out a spreadsheet\n\nLearning goal: identify when a function’s goal is too complex and could be split into subfunctions that are easier to develop and test.\n\nInstruction: Read the following functions and note what you expect they will output. We have purposely used vague function names in this case - not our typical recommendation! When you are ready, click “Show output”.\nFunction 1\n\napple &lt;- 'green'\n\nf &lt;- function(x) {\n  nchar(x)\n}\n\nf(apple)\n\n\nShow output\n\n\n[1] 5\n\n\nFunction 2\n\napple &lt;- 'gala'\n\nf &lt;- function(x, times) {\n  rep(x, times = times)\n}\n\nf(apple, 10)\n\n\nShow output\n\n\n [1] \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\" \"gala\"\n\n\nFunction 3\n\napple &lt;- 'mcintosh'\npotato &lt;- 'russet'\n\nf &lt;- function(apple, potato) {\n  data.frame(apples = apple, potatoes = potato)\n}\n\nf(potato = potato)\n\n\nShow output\n\n\nError in f(potato = potato): argument \"apple\" is missing, with no default"
  },
  {
    "objectID": "exercises/functions/02-functions-development.html",
    "href": "exercises/functions/02-functions-development.html",
    "title": "Developing functions",
    "section": "",
    "text": "The setup:\n\nSetup the function\n\nMake a function script in the R/ directory named function_name.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_function_name.R.\nLoad any required packages (library(package))\nSource the function (source('R/function_name.R))\nLoad example data and/or arguments for the function\n\n\n\nWe’ll use the following approach for developing functions:\n\nFill in the body of the function, state the returned object, add arguments in the function script (R/function_name.R)\nAdd a test that uses the function with the example data\nRun the entire test script\nCheck the output, note required changes, errors, warnings, etc\nHead back to the function script to make changes\nRun the entire test script\n\nCheck the test output, tweak the function, repeat!\nThis process of editing two files might seem like extra work, but we recommend separating the function’s definition from the bits of code we use to test it. Think of the test script as your isolated playground to test and develop the function and your function script as the sheltered environment where you carefully tweak the function. You can also use this test script as a place to develop the function using the same inputs and arguments as the function. Keeping these parts separate can help to avoid mysterious errors and general chaos!\n\n\nInstruction: setup your function script and test script for your new function prepare_csv().\nUse whichever function you prefer for reading CSVs (eg. data.table::fread(), or readr::read_csv()). For cleaning column names, we’ll use the function clean_names() from the {janitor} package.\nNote: since we are writing a function to read the data in, we’ll test the function with the data’s path instead. We’ll use the example dataset for adult and chick counts described in the README. For example:\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\nUse the section “Development” in your test script (tests/test_prepare_csv.R) to develop the body of your function. Use path_counts and the function for reading CSVs, then the clean_names() function from the {janitor} package. Add the code to the body of your function prepare_csv() and test!\n\nHints\n\nThis function has two steps: reading a CSV from a path and cleaning the column names.\nBoth of these steps are going to use functions from external packages, so we only have to put them together.\nThe function’s arguments should include the path to the CSV.\nFirst, take the path and pass it to the function for reading CSVs.\nThen, pass the data.frame returned to the clean_names() function from the {janitor} package."
  },
  {
    "objectID": "exercises/functions/02-functions-development.html#setup",
    "href": "exercises/functions/02-functions-development.html#setup",
    "title": "Developing functions",
    "section": "",
    "text": "The setup:\n\nSetup the function\n\nMake a function script in the R/ directory named function_name.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_function_name.R.\nLoad any required packages (library(package))\nSource the function (source('R/function_name.R))\nLoad example data and/or arguments for the function"
  },
  {
    "objectID": "exercises/functions/02-functions-development.html#approach",
    "href": "exercises/functions/02-functions-development.html#approach",
    "title": "Developing functions",
    "section": "",
    "text": "We’ll use the following approach for developing functions:\n\nFill in the body of the function, state the returned object, add arguments in the function script (R/function_name.R)\nAdd a test that uses the function with the example data\nRun the entire test script\nCheck the output, note required changes, errors, warnings, etc\nHead back to the function script to make changes\nRun the entire test script\n\nCheck the test output, tweak the function, repeat!\nThis process of editing two files might seem like extra work, but we recommend separating the function’s definition from the bits of code we use to test it. Think of the test script as your isolated playground to test and develop the function and your function script as the sheltered environment where you carefully tweak the function. You can also use this test script as a place to develop the function using the same inputs and arguments as the function. Keeping these parts separate can help to avoid mysterious errors and general chaos!"
  },
  {
    "objectID": "exercises/functions/02-functions-development.html#exercises-function-development",
    "href": "exercises/functions/02-functions-development.html#exercises-function-development",
    "title": "Developing functions",
    "section": "",
    "text": "Instruction: setup your function script and test script for your new function prepare_csv().\nUse whichever function you prefer for reading CSVs (eg. data.table::fread(), or readr::read_csv()). For cleaning column names, we’ll use the function clean_names() from the {janitor} package.\nNote: since we are writing a function to read the data in, we’ll test the function with the data’s path instead. We’ll use the example dataset for adult and chick counts described in the README. For example:\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\nUse the section “Development” in your test script (tests/test_prepare_csv.R) to develop the body of your function. Use path_counts and the function for reading CSVs, then the clean_names() function from the {janitor} package. Add the code to the body of your function prepare_csv() and test!\n\nHints\n\nThis function has two steps: reading a CSV from a path and cleaning the column names.\nBoth of these steps are going to use functions from external packages, so we only have to put them together.\nThe function’s arguments should include the path to the CSV.\nFirst, take the path and pass it to the function for reading CSVs.\nThen, pass the data.frame returned to the clean_names() function from the {janitor} package."
  },
  {
    "objectID": "exercises/functions/03-functions-checks.html",
    "href": "exercises/functions/03-functions-checks.html",
    "title": "Checks",
    "section": "",
    "text": "Checks in functions are used to ensure inputs and arguments are as expected, to handle errors and to provide informative error messages to users. In this workshop we are going to focus on writing checks with the function stopifnot because it is simple and succinct. It was recently improved in R 4.0.0 when the R development team added the option to provide a more informative error which will help us clearly communicate with our users (and ourselves).\nstopifnot() expects a logical statement that you are expecting to be true, and, if not, stopifnot() will throw an error.\nTo build logical statements, you may find following list of functions useful:\n\nclass checks\n\n\nis.integer, is.numeric, is.data.frame, is.character\n\n\n\nnulls, NAs, NaNs\n\n\nis.null, is.na, is.nan\n\n\n\ndimensions\n\n\nlength, nrow, ncol, dim\n\n\n\nequality, comparisons\n\n\n== (equals)\n\n&gt;, &lt;, &gt;=, &lt;= (less than, greater than, less than or equal to, greater than or equal to)\nidentical\n\n\nnegation\n\n!\n\n\ndirectories, files\n\nfile.exists\ndir\n\nbasename, dirname\n\n\n\n\n\n\nInstruction: write a series of logical statements for the following objects then consult the solution to check your work. Focus only on the logical statement for now, we’ll use stopifnot in the following exercises.\nNumber\n\ncheck if x is a numeric, integer, double\ncheck if x is of length 1\ncheck if x is greater than 0\n\n\nx &lt;- 10\n\n\nShow solution\n\nis.numeric(x)\n\n[1] TRUE\n\nis.integer(x)\n\n[1] FALSE\n\nis.double(x)\n\n[1] TRUE\n\nlength(x) == 1\n\n[1] TRUE\n\nx &gt; 0\n\n[1] TRUE\n\n\ndata.frame\n\ncheck if DF is a data.frame, list, matrix\ncheck if DF has two columns and three rows\ncheck if DF’s column numbers is an integer, double, numeric\n\n\nDF &lt;- data.frame(colors = c('red', 'green', 'blue'), numbers = c(42.1, 2L, 10))\n\n\nShow solution\n\nis.data.frame(DF)\n\n[1] TRUE\n\nis.list(DF)\n\n[1] TRUE\n\nis.matrix(DF)\n\n[1] FALSE\n\nncol(DF) == 2\n\n[1] TRUE\n\nnrow(DF) == 3\n\n[1] TRUE\n\nis.integer(DF$numbers)\n\n[1] FALSE\n\nis.double(DF$numbers)\n\n[1] TRUE\n\nis.numeric(DF$numbers)\n\n[1] TRUE\n\n\n\nFirst, write a function to plot a data.frame, providing columns names for the data to be plotted on the x and y axes. Use the example data that we prepared with our prepare_csv function.\nFollow the steps from our approach to developing functions:\n\nSetup the function\n\nMake a function script in the R/ directory named plot_xy.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_plot_xy.R.\nLoad any required packages (library(package))\nSource the function (source('R/function.R))\nLoad example data and/or arguments for the function\n\n\n\nAdd a new section ‘Development’ in the test script (tests/test_plot_xy.R) to develop the body of your function. Since we have already built a function for preparing CSV files, use it in your test script, and your choice of base R plotting or {ggplot2}. Add the code to your function plot_xy() and test!\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\nprep_counts &lt;- prepare_csv(path_counts)\n\n\nHints: base R\n\nThis function has one step: taking an input data.frame and generating a plot.\nThe function’s arguments should include the data.frame, as well as the column names for the X and Y axes.\nPass the data.frame’s to the plot function’s x and y arguments with the [[ syntax, eg. DF[[x_column]]\n\nHints: ggplot2\n\nThis function has one step: taking an input data.frame and generating a plot.\nThe function’s arguments should include the data.frame, as well as the column names for the X and Y axes.\nThere are two main options:\n\nPass the data.frame’s to ggplot’s data argument then use the .data[[col]] syntax to wrap the x_col and y_col arguments (quoted column names) within aes.\nPass the data.frame’s to ggplot’s data argument then use the {{ }} “embrace operator” to wrap the x_col and y_col arguments (unquoted* column names) within aes.\n\nSee the ?aes help page and the example with the “embrace operator”, and more details here: https://ggplot2.tidyverse.org/articles/ggplot2-in-packages.html#using-aes-and-vars-in-a-package-function\n\nInstruction: add the following checks to your plot_xy() function and test them in your test script (tests/test_plot_xy.R).\n\ncheck if the columns provided to the x_col and y_col arguments exist in the data.frame (quoted column name arguments)\ncheck if the plot generated is of class ggplot before returning (ggplot2)\n\n\nHints: unquoted column names with ggplot2\n\nIf you opted to use unquoted column names, it might be trickier to check if column names exist in the data.frame. However - this check is already covered by ggplot() anyways.\nIn case you wanted to add this, on top of the internal ggplot2 check, try something like this:\nas_name(enquo(x_col)) %in% colnames(DT)\nThese are {rlang} functions, part of the “tidy eval tools” and examples of metaprogramming. The Advanced R book has a detailed section on this concept.\n\n\nInstruction: add the following checks to your prepare_csv() function and test them in your test script (tests/test_prepare_csv.R).\n\ncheck if the path points to a file that exists\nbefore returning the object, check that it is a data.frame\n\nInstruction: Write more informative errors for your stopifnot() checks using the following syntax: stopifnot(\"error message\" = logical_statement). Think about your user (either someone else or future you) - what would help them understand and resolve this error?\nFor example,\nstopifnot(\"x is not a numeric\" = is.numeric(x))\n\nInstruction: using the same approach you used for x_col and y_col, add a color_col argument to plot_xy()."
  },
  {
    "objectID": "exercises/functions/03-functions-checks.html#exercises-function-checks",
    "href": "exercises/functions/03-functions-checks.html#exercises-function-checks",
    "title": "Checks",
    "section": "",
    "text": "Instruction: write a series of logical statements for the following objects then consult the solution to check your work. Focus only on the logical statement for now, we’ll use stopifnot in the following exercises.\nNumber\n\ncheck if x is a numeric, integer, double\ncheck if x is of length 1\ncheck if x is greater than 0\n\n\nx &lt;- 10\n\n\nShow solution\n\nis.numeric(x)\n\n[1] TRUE\n\nis.integer(x)\n\n[1] FALSE\n\nis.double(x)\n\n[1] TRUE\n\nlength(x) == 1\n\n[1] TRUE\n\nx &gt; 0\n\n[1] TRUE\n\n\ndata.frame\n\ncheck if DF is a data.frame, list, matrix\ncheck if DF has two columns and three rows\ncheck if DF’s column numbers is an integer, double, numeric\n\n\nDF &lt;- data.frame(colors = c('red', 'green', 'blue'), numbers = c(42.1, 2L, 10))\n\n\nShow solution\n\nis.data.frame(DF)\n\n[1] TRUE\n\nis.list(DF)\n\n[1] TRUE\n\nis.matrix(DF)\n\n[1] FALSE\n\nncol(DF) == 2\n\n[1] TRUE\n\nnrow(DF) == 3\n\n[1] TRUE\n\nis.integer(DF$numbers)\n\n[1] FALSE\n\nis.double(DF$numbers)\n\n[1] TRUE\n\nis.numeric(DF$numbers)\n\n[1] TRUE\n\n\n\nFirst, write a function to plot a data.frame, providing columns names for the data to be plotted on the x and y axes. Use the example data that we prepared with our prepare_csv function.\nFollow the steps from our approach to developing functions:\n\nSetup the function\n\nMake a function script in the R/ directory named plot_xy.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_plot_xy.R.\nLoad any required packages (library(package))\nSource the function (source('R/function.R))\nLoad example data and/or arguments for the function\n\n\n\nAdd a new section ‘Development’ in the test script (tests/test_plot_xy.R) to develop the body of your function. Since we have already built a function for preparing CSV files, use it in your test script, and your choice of base R plotting or {ggplot2}. Add the code to your function plot_xy() and test!\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\nprep_counts &lt;- prepare_csv(path_counts)\n\n\nHints: base R\n\nThis function has one step: taking an input data.frame and generating a plot.\nThe function’s arguments should include the data.frame, as well as the column names for the X and Y axes.\nPass the data.frame’s to the plot function’s x and y arguments with the [[ syntax, eg. DF[[x_column]]\n\nHints: ggplot2\n\nThis function has one step: taking an input data.frame and generating a plot.\nThe function’s arguments should include the data.frame, as well as the column names for the X and Y axes.\nThere are two main options:\n\nPass the data.frame’s to ggplot’s data argument then use the .data[[col]] syntax to wrap the x_col and y_col arguments (quoted column names) within aes.\nPass the data.frame’s to ggplot’s data argument then use the {{ }} “embrace operator” to wrap the x_col and y_col arguments (unquoted* column names) within aes.\n\nSee the ?aes help page and the example with the “embrace operator”, and more details here: https://ggplot2.tidyverse.org/articles/ggplot2-in-packages.html#using-aes-and-vars-in-a-package-function\n\nInstruction: add the following checks to your plot_xy() function and test them in your test script (tests/test_plot_xy.R).\n\ncheck if the columns provided to the x_col and y_col arguments exist in the data.frame (quoted column name arguments)\ncheck if the plot generated is of class ggplot before returning (ggplot2)\n\n\nHints: unquoted column names with ggplot2\n\nIf you opted to use unquoted column names, it might be trickier to check if column names exist in the data.frame. However - this check is already covered by ggplot() anyways.\nIn case you wanted to add this, on top of the internal ggplot2 check, try something like this:\nas_name(enquo(x_col)) %in% colnames(DT)\nThese are {rlang} functions, part of the “tidy eval tools” and examples of metaprogramming. The Advanced R book has a detailed section on this concept.\n\n\nInstruction: add the following checks to your prepare_csv() function and test them in your test script (tests/test_prepare_csv.R).\n\ncheck if the path points to a file that exists\nbefore returning the object, check that it is a data.frame\n\nInstruction: Write more informative errors for your stopifnot() checks using the following syntax: stopifnot(\"error message\" = logical_statement). Think about your user (either someone else or future you) - what would help them understand and resolve this error?\nFor example,\nstopifnot(\"x is not a numeric\" = is.numeric(x))\n\nInstruction: using the same approach you used for x_col and y_col, add a color_col argument to plot_xy()."
  },
  {
    "objectID": "exercises/functions/04-functions-control-flow.html",
    "href": "exercises/functions/04-functions-control-flow.html",
    "title": "Controlling flow",
    "section": "",
    "text": "Conditionals in functions are used to modify how a function runs, determine what output to return or to return warnings and errors depending on logical statements. We’ll look at if and else, as well as functions for combining multiple logical statements. if and else can be used instead of stopifnot, usually in cases where logical statements are more complex or a more elaborate error message is required.\nThe following functions are essential for controlling flow in R:\n\n\nif else (and else if)\n\n||, && combine multiple logical expressions and short-circuit when || sees the first true and && sees the first false\n\n|, & are vectorized and cannot be directly used in an if statement unless you also use any, all\n\n\n\n\nInstruction: write a series of if else conditional statements for the following objects then consult the solution to check your work.\nNumeric\n\nif x is a numeric, return x as a character\nprint ‘positive’ if x is greater than 0, else print ‘negative’ if x is less than 0\nif x / 3 returns a remainder (hint: see ?`%%`), print the remainder\n\n\nx &lt;- 10\n\n\nShow solution\n\nif (is.numeric(x)) {\n    as.character(x)\n}\n\n[1] \"10\"\n\nif (x &gt; 0) {\n    print('positive')\n} else if (x &lt; 0) {\n    print('negative')\n}\n\n[1] \"positive\"\n\nif (x %% 3) {\n    print(x %% 3)\n}\n\n[1] 1\n\n\ndata.frame\n\nreturn the column ‘colors’ if it exists in DF\nif type is ‘histogram’, plot a histogram of DF’s column ‘numbers’\n\n\nDF &lt;- data.frame(colors = c('red', 'green', 'blue'), numbers = c(42.1, 2L, 10))\ntype &lt;- 'histogram'\n\n\nShow solution\n\nif ('colors' %in% colnames(DF)) {\n    DF['colors']\n}\n\n  colors\n1    red\n2  green\n3   blue\n\nif (type == 'histogram') {\n    hist(DF$numbers)\n}\n\n\n\n\n\nNext we’ll write a new function that processes our prepared CSV files.\nLet’s say we wanted to summarize the number of adults and chicks counted by island. First, write a function to sum the ‘adults’ and ‘chicks’ columns in the data.frame. Use the example data that we prepared with our prepare_csv function.\nFollow the steps from our approach to developing functions:\n\nSetup the function\n\nMake a function script in the R/ directory named sum_counts.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_sum_counts.R.\nLoad any required packages (library(package))\nSource the function (source('R/function.R))\nLoad example data and/or arguments for the function\n\n\n\nSince we have already built a function for preparing CSV files, use it in your test script:\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\nprep_counts &lt;- prepare_csv(path_counts)\n\nAdd a new section ‘Development’ in the test script (tests/test_sum_counts.R) to develop the body of your function. Use the prepared CSV data as shown above, add the code to your function sum_counts() and test!\n\nHints: by\n\n{data.table} has an argument by\n{dplyr} has an alternative to group_by for passing character column names called group_by_at\nNext steps:\n\ndecide if it is better for you (or a potential user) to set the columns that we calculate the sum by inside the function (fixed) or as an argument (flexible) - try both options\nadd a filter argument filter_gt_year (default set to NULL) and when it is not NULL, filter the data only greater than the value provided\n\nTest these as you go in your test script tests/test_sum_counts.R.\n\nHints: defaults\n\nTo set a function’s argument to default, use the syntax argument = default_value. For example:\nfunction(x = 1, y = 2, z = 3) {\n\n}\n\nAdd checks to your functions plot_xy() and sum_counts():\n\nuse the arguments x_col and y_col to also update the axis labels in plot_xy() (base R)\nuse a new argument color_col to provide a column in plot_xy()\n\nbuild a plot function that plots the output of sum_counts()"
  },
  {
    "objectID": "exercises/functions/04-functions-control-flow.html#exercises-function-conditionals",
    "href": "exercises/functions/04-functions-control-flow.html#exercises-function-conditionals",
    "title": "Controlling flow",
    "section": "",
    "text": "Instruction: write a series of if else conditional statements for the following objects then consult the solution to check your work.\nNumeric\n\nif x is a numeric, return x as a character\nprint ‘positive’ if x is greater than 0, else print ‘negative’ if x is less than 0\nif x / 3 returns a remainder (hint: see ?`%%`), print the remainder\n\n\nx &lt;- 10\n\n\nShow solution\n\nif (is.numeric(x)) {\n    as.character(x)\n}\n\n[1] \"10\"\n\nif (x &gt; 0) {\n    print('positive')\n} else if (x &lt; 0) {\n    print('negative')\n}\n\n[1] \"positive\"\n\nif (x %% 3) {\n    print(x %% 3)\n}\n\n[1] 1\n\n\ndata.frame\n\nreturn the column ‘colors’ if it exists in DF\nif type is ‘histogram’, plot a histogram of DF’s column ‘numbers’\n\n\nDF &lt;- data.frame(colors = c('red', 'green', 'blue'), numbers = c(42.1, 2L, 10))\ntype &lt;- 'histogram'\n\n\nShow solution\n\nif ('colors' %in% colnames(DF)) {\n    DF['colors']\n}\n\n  colors\n1    red\n2  green\n3   blue\n\nif (type == 'histogram') {\n    hist(DF$numbers)\n}\n\n\n\n\n\nNext we’ll write a new function that processes our prepared CSV files.\nLet’s say we wanted to summarize the number of adults and chicks counted by island. First, write a function to sum the ‘adults’ and ‘chicks’ columns in the data.frame. Use the example data that we prepared with our prepare_csv function.\nFollow the steps from our approach to developing functions:\n\nSetup the function\n\nMake a function script in the R/ directory named sum_counts.R\n\nWrite the function’s skeleton (name, arguments, curly braces)\n\n\nSetup the test script\n\nMake a corresponding test script in the tests/ directory named test_sum_counts.R.\nLoad any required packages (library(package))\nSource the function (source('R/function.R))\nLoad example data and/or arguments for the function\n\n\n\nSince we have already built a function for preparing CSV files, use it in your test script:\n\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\nprep_counts &lt;- prepare_csv(path_counts)\n\nAdd a new section ‘Development’ in the test script (tests/test_sum_counts.R) to develop the body of your function. Use the prepared CSV data as shown above, add the code to your function sum_counts() and test!\n\nHints: by\n\n{data.table} has an argument by\n{dplyr} has an alternative to group_by for passing character column names called group_by_at\nNext steps:\n\ndecide if it is better for you (or a potential user) to set the columns that we calculate the sum by inside the function (fixed) or as an argument (flexible) - try both options\nadd a filter argument filter_gt_year (default set to NULL) and when it is not NULL, filter the data only greater than the value provided\n\nTest these as you go in your test script tests/test_sum_counts.R.\n\nHints: defaults\n\nTo set a function’s argument to default, use the syntax argument = default_value. For example:\nfunction(x = 1, y = 2, z = 3) {\n\n}\n\nAdd checks to your functions plot_xy() and sum_counts():\n\nuse the arguments x_col and y_col to also update the axis labels in plot_xy() (base R)\nuse a new argument color_col to provide a column in plot_xy()\n\nbuild a plot function that plots the output of sum_counts()"
  },
  {
    "objectID": "exercises/functions/05-functions-bonus.html",
    "href": "exercises/functions/05-functions-bonus.html",
    "title": "Bonus",
    "section": "",
    "text": "If you are already done all the previous exercises and we have extra time, - or on your own after the workshop, try these additional exercises.\n\ntestthat is one of the main R packages for testing functions (specifically designed for R packages).\nBriefly, the approach is to write test scripts that have a series of expectations for the functions. For example, does the function return a particular class or does the function return an error when expected?\n\nlibrary(testthat)\ngrep('expect_', lsf.str('package:testthat'), value = TRUE)\n\n [1] \"expect_condition\"          \"expect_cpp_tests_pass\"    \n [3] \"expect_equal\"              \"expect_equal_to_reference\"\n [5] \"expect_equivalent\"         \"expect_error\"             \n [7] \"expect_failure\"            \"expect_false\"             \n [9] \"expect_gt\"                 \"expect_gte\"               \n[11] \"expect_identical\"          \"expect_invisible\"         \n[13] \"expect_is\"                 \"expect_known_hash\"        \n[15] \"expect_known_output\"       \"expect_known_value\"       \n[17] \"expect_length\"             \"expect_less_than\"         \n[19] \"expect_lt\"                 \"expect_lte\"               \n[21] \"expect_mapequal\"           \"expect_match\"             \n[23] \"expect_message\"            \"expect_more_than\"         \n[25] \"expect_named\"              \"expect_no_condition\"      \n[27] \"expect_no_error\"           \"expect_no_match\"          \n[29] \"expect_no_message\"         \"expect_no_warning\"        \n[31] \"expect_null\"               \"expect_output\"            \n[33] \"expect_output_file\"        \"expect_reference\"         \n[35] \"expect_s3_class\"           \"expect_s4_class\"          \n[37] \"expect_setequal\"           \"expect_silent\"            \n[39] \"expect_snapshot\"           \"expect_snapshot_error\"    \n[41] \"expect_snapshot_file\"      \"expect_snapshot_output\"   \n[43] \"expect_snapshot_value\"     \"expect_snapshot_warning\"  \n[45] \"expect_success\"            \"expect_that\"              \n[47] \"expect_true\"               \"expect_type\"              \n[49] \"expect_vector\"             \"expect_visible\"           \n[51] \"expect_warning\"           \n\n\nSee the reference and vignettes here: https://testthat.r-lib.org/\n\nTry iterating functions over lists or groups using the apply family or purrr::map family.\n\nhttps://r4ds.had.co.nz/iteration.html\nhttps://swcarpentry.github.io/r-novice-inflammation/15-supp-loops-in-depth.html\nhttps://purrr.tidyverse.org/\n\nRead about object-oriented programming classes in R:\n\nS3\nR6\nS4\n\nTry the lexical scoping exercises here:\n\nhttps://adv-r.hadley.nz/functions.html#lexical-scoping"
  },
  {
    "objectID": "exercises/functions/05-functions-bonus.html#testthat",
    "href": "exercises/functions/05-functions-bonus.html#testthat",
    "title": "Bonus",
    "section": "",
    "text": "testthat is one of the main R packages for testing functions (specifically designed for R packages).\nBriefly, the approach is to write test scripts that have a series of expectations for the functions. For example, does the function return a particular class or does the function return an error when expected?\n\nlibrary(testthat)\ngrep('expect_', lsf.str('package:testthat'), value = TRUE)\n\n [1] \"expect_condition\"          \"expect_cpp_tests_pass\"    \n [3] \"expect_equal\"              \"expect_equal_to_reference\"\n [5] \"expect_equivalent\"         \"expect_error\"             \n [7] \"expect_failure\"            \"expect_false\"             \n [9] \"expect_gt\"                 \"expect_gte\"               \n[11] \"expect_identical\"          \"expect_invisible\"         \n[13] \"expect_is\"                 \"expect_known_hash\"        \n[15] \"expect_known_output\"       \"expect_known_value\"       \n[17] \"expect_length\"             \"expect_less_than\"         \n[19] \"expect_lt\"                 \"expect_lte\"               \n[21] \"expect_mapequal\"           \"expect_match\"             \n[23] \"expect_message\"            \"expect_more_than\"         \n[25] \"expect_named\"              \"expect_no_condition\"      \n[27] \"expect_no_error\"           \"expect_no_match\"          \n[29] \"expect_no_message\"         \"expect_no_warning\"        \n[31] \"expect_null\"               \"expect_output\"            \n[33] \"expect_output_file\"        \"expect_reference\"         \n[35] \"expect_s3_class\"           \"expect_s4_class\"          \n[37] \"expect_setequal\"           \"expect_silent\"            \n[39] \"expect_snapshot\"           \"expect_snapshot_error\"    \n[41] \"expect_snapshot_file\"      \"expect_snapshot_output\"   \n[43] \"expect_snapshot_value\"     \"expect_snapshot_warning\"  \n[45] \"expect_success\"            \"expect_that\"              \n[47] \"expect_true\"               \"expect_type\"              \n[49] \"expect_vector\"             \"expect_visible\"           \n[51] \"expect_warning\"           \n\n\nSee the reference and vignettes here: https://testthat.r-lib.org/"
  },
  {
    "objectID": "exercises/functions/05-functions-bonus.html#iteration",
    "href": "exercises/functions/05-functions-bonus.html#iteration",
    "title": "Bonus",
    "section": "",
    "text": "Try iterating functions over lists or groups using the apply family or purrr::map family.\n\nhttps://r4ds.had.co.nz/iteration.html\nhttps://swcarpentry.github.io/r-novice-inflammation/15-supp-loops-in-depth.html\nhttps://purrr.tidyverse.org/"
  },
  {
    "objectID": "exercises/functions/05-functions-bonus.html#object-oriented-programming",
    "href": "exercises/functions/05-functions-bonus.html#object-oriented-programming",
    "title": "Bonus",
    "section": "",
    "text": "Read about object-oriented programming classes in R:\n\nS3\nR6\nS4"
  },
  {
    "objectID": "exercises/functions/05-functions-bonus.html#lexical-scoping",
    "href": "exercises/functions/05-functions-bonus.html#lexical-scoping",
    "title": "Bonus",
    "section": "",
    "text": "Try the lexical scoping exercises here:\n\nhttps://adv-r.hadley.nz/functions.html#lexical-scoping"
  },
  {
    "objectID": "exercises/projects.html",
    "href": "exercises/projects.html",
    "title": "Project setup",
    "section": "",
    "text": "We are going to practice setting up an RStudio Project to get oriented with the process. Please follow along with these steps and remember, if you are lost use your red sticky note or raise your hand and someone will come help you.\n\nOpen RStudio and navigate to the upper right hand corner\nClick on the drop-down menu and select the first option: “New Project…”\n\nAlternatively, navigate to the top left corner, click “File”, and then click “New Project…”\n\nSelect “New Directory”, then “New Project”\nName the directory “2023-CSEE-workshop-projects” and navigate to the subdirectory in your computer where you want the folder to appear\nMinimize RStudio and using your file navigation system, navigate to the place on the computer where your project is saved (e.g., Downloads)\nOpen the folder (you should see one file in there, an .Rproj file with the name you gave the folder)\n\nCreate five folders within your project folder:\n\nraw-data\noutput\ngraphics\nscripts\nR\n\n\nDownload the data from this link and move it into the raw-data/ folder\nOpen your project in RStudio\nOpen an empty file and save it as “README.md”\nFill in your README file using one of the templates found under the “Templates” sidebar\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many questions you may not have the answers to for this exercise - either make them up or ignore them! Do your best to answer what you can (while thinking about how you may alter/use this format in your own life/projects)\n\n\n\n(If you have time) Share this directory with a neighbour as a zip file and see if you can each open and work with each other’s project"
  },
  {
    "objectID": "exercises/projects.html#exercises-project-setup",
    "href": "exercises/projects.html#exercises-project-setup",
    "title": "Project setup",
    "section": "",
    "text": "We are going to practice setting up an RStudio Project to get oriented with the process. Please follow along with these steps and remember, if you are lost use your red sticky note or raise your hand and someone will come help you.\n\nOpen RStudio and navigate to the upper right hand corner\nClick on the drop-down menu and select the first option: “New Project…”\n\nAlternatively, navigate to the top left corner, click “File”, and then click “New Project…”\n\nSelect “New Directory”, then “New Project”\nName the directory “2023-CSEE-workshop-projects” and navigate to the subdirectory in your computer where you want the folder to appear\nMinimize RStudio and using your file navigation system, navigate to the place on the computer where your project is saved (e.g., Downloads)\nOpen the folder (you should see one file in there, an .Rproj file with the name you gave the folder)\n\nCreate five folders within your project folder:\n\nraw-data\noutput\ngraphics\nscripts\nR\n\n\nDownload the data from this link and move it into the raw-data/ folder\nOpen your project in RStudio\nOpen an empty file and save it as “README.md”\nFill in your README file using one of the templates found under the “Templates” sidebar\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many questions you may not have the answers to for this exercise - either make them up or ignore them! Do your best to answer what you can (while thinking about how you may alter/use this format in your own life/projects)\n\n\n\n(If you have time) Share this directory with a neighbour as a zip file and see if you can each open and work with each other’s project"
  },
  {
    "objectID": "exercises/renv.html",
    "href": "exercises/renv.html",
    "title": "{renv}",
    "section": "",
    "text": "{renv} helps us create more reproducible and shareable projects by allowing us to run code on different machines, across time, by different people. It essentially takes the packages used in your project and records their versions in a file that you can update as you develop the project. When you come back to your project, or share it with someone else, they are using the same versions of packages and R that you were.\nThe three main functions used when working with {renv} are:\n\ninit() for setting up project to use {renv} for the first time\nsnapshot() for recording versions of packages currently installed in the project in the file renv.lock\nrestore() for restoring a project’s package versions to match the versions recorded by {renv} from the file renv.lock\n\nNote: {renv} will not install a different version of R, or any other system dependencies that are required. However, {renv} will warn if the renv.lock lists a version of R that differs from your current version of R. For system dependencies that are required but not installed, R will fail.\n\n\nThe first time you set up a project with {renv}, use the function init(). init() writes a file to the root of your project called renv.lock that tracks the versions and dependencies of R packages installed and the version of R itself. This is a simple text file, so feel free to open it up (but don’t edit) in a text editor.\n\n\nInstruction: in the console of your project, initialize {renv} with init().\n\n\n\n\nAs you work on your project and add or update packages, record your packages used and their version with snapshot(). This will edit the renv.lock file with new versions and packages that describe the package’s environment.\nYou can always check the status of your {renv} project with status() to see which packages are installed but not recorded, or which packages are recorded but no longer used.\n\n\nInstruction: install a new package with install.packages() then run status(). Follow {renv}’s recommendations to either save (snapshot()) or discard (restore()) these potential changes to your project’s environment.\n\n\n\n\nsnapshot() is used to save the current state of your project while restore() to recover the state of your project as it was last saved. When you download someone else’s project, you can use restore() to retrieve and install the versions of packages in the project that were recorded.\nBefore sharing your project (and regularly while you work even if not sharing), make sure your project is up to date by running snapshot().\nTo restore from an renv.lock file, simply run restore().\n\n\nGit is really helpful when working with {renv} because we can track the renv.lock file’s history for changes through time. See these examples:\n\nrobitalec/targets-issa: renv.lock and history\nrobitalec/prepare-locs: renv.lock and history\n\nUse renv::update() to update the packages in your lock file.\nYou can also use {renv} to track Python dependencies required by your project too. See this vignette for more details: https://rstudio.github.io/renv/articles/python.html"
  },
  {
    "objectID": "exercises/renv.html#setup",
    "href": "exercises/renv.html#setup",
    "title": "{renv}",
    "section": "",
    "text": "The first time you set up a project with {renv}, use the function init(). init() writes a file to the root of your project called renv.lock that tracks the versions and dependencies of R packages installed and the version of R itself. This is a simple text file, so feel free to open it up (but don’t edit) in a text editor.\n\n\nInstruction: in the console of your project, initialize {renv} with init()."
  },
  {
    "objectID": "exercises/renv.html#usage",
    "href": "exercises/renv.html#usage",
    "title": "{renv}",
    "section": "",
    "text": "As you work on your project and add or update packages, record your packages used and their version with snapshot(). This will edit the renv.lock file with new versions and packages that describe the package’s environment.\nYou can always check the status of your {renv} project with status() to see which packages are installed but not recorded, or which packages are recorded but no longer used.\n\n\nInstruction: install a new package with install.packages() then run status(). Follow {renv}’s recommendations to either save (snapshot()) or discard (restore()) these potential changes to your project’s environment."
  },
  {
    "objectID": "exercises/renv.html#sharing",
    "href": "exercises/renv.html#sharing",
    "title": "{renv}",
    "section": "",
    "text": "snapshot() is used to save the current state of your project while restore() to recover the state of your project as it was last saved. When you download someone else’s project, you can use restore() to retrieve and install the versions of packages in the project that were recorded.\nBefore sharing your project (and regularly while you work even if not sharing), make sure your project is up to date by running snapshot().\nTo restore from an renv.lock file, simply run restore().\n\n\nGit is really helpful when working with {renv} because we can track the renv.lock file’s history for changes through time. See these examples:\n\nrobitalec/targets-issa: renv.lock and history\nrobitalec/prepare-locs: renv.lock and history\n\nUse renv::update() to update the packages in your lock file.\nYou can also use {renv} to track Python dependencies required by your project too. See this vignette for more details: https://rstudio.github.io/renv/articles/python.html"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html",
    "href": "exercises/targets/01-targets-setup.html",
    "title": "Setup",
    "section": "",
    "text": "This set of exercises are centered around an approach to setting up your project using {targets}.\nA {targets} pipeline is declared in the _targets.R file in the root of your project’s directory. We’ll also set up a central place to put all our of required packages that helps us keep track of them, and will later make it easier to set up {renv}.\n\n\nThe _targets.R is where we declare our {targets} pipeline. It is an R file saved in the root directory of your project. The generalized structure of a _targets.R file is:\n\nLoad packages\nSet options and variables\nEnd in a list of targets\n\nNote: our _targets.R script must end in a list of targets.\nA simple example script could be:\n_targets.R\nlibrary(targets)\n\ndata(mtcars)\n\nc(\n    tar_target(\n        histogram_mpg, \n        hist(mtcars$mpg)\n    )\n)\n\n\nInstruction: make a new R script in the root directory of your project named _targets.R. At the top, load {targets} and at the bottom put an empty list c( ).\n\n\n\n\nTo declare which packages we need across all our functions, we have two main options:\n\nList them with library() calls at the top of our _targets.R script\nList them all in a R/packages.R script and source that script.\n\nWe prefer the second approach because it keeps our _targets.R tidier, and helps us setup {renv} package versioning and {conflicted} later. You might find your list of packages gets long, and this gets them out of our way. You can also use this script when you are exploring your results later - source the packages.R script and you will have all your required packages.\n\n\nInstruction: make a new script called packages.R in the R/ directory. Go through your function test scripts from previous exercises and add all the packages you used to the packages.R file with a library() call for each package on a separate line.\nFor example:\nR/packages.R\nlibrary(targets)\nlibrary(data.table)\nlibrary(ggplot2)\nNow, to make those packages available to the {targets} pipeline, we need to load them in our _targets.R script.\nInstruction: add a command to source the R/packages.R in your _targets.R script.\nFor example:\n_targets.R\n# Packages\nsource(file.path('R', 'packages.R'))\n\n\n\n\nTo make functions available to the {targets} pipeline we also need to also source them in the _targets.R script.\nA shortcut for sourcing all the R files in a specific folder is provided by {targets}: tar_source(). We will use this to source our packages and functions in one step.\nNote: something to be careful about now that we are sourcing the R/ directory - you shouldn’t have any calls to {targets} functions like tar_read(), tar_load() or tar_make() in the script in the R/ directory. This is an impossible circular dependency - you are load or reading a target while you are also in the process of making it. Put any tar_read(), tar_load() or tar_make() calls elsewhere.\n\n\nInstruction: replace the above source('R/packages.R') command with the function tar_source() pointing to the R/ directory with your packages.R script and the functions you developed in the previous exercises.\nNote: you will need to load {targets} to run the tar_source() function.\nFor example:\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n\n\n\nWe’ll come back to setting {targets} options later, but let’s set up a spot in our _targets.R script reserved for options. The relevant {targets} function is tar_option_set(). It can set global options for each target, such as the file format to save them in and when they should be cued to rerun. In addition, you can use tar_option_set() to manage options related to overall pipeline runs such as what to do when a targets hits an error.\nThe full reference page for tar_option_set() is available here: https://docs.ropensci.org/targets/reference/tar_option_set.html\n\n\nInstruction: make a new section in your _targets.R script named Options, and put an empty call to tar_option_set() there.\nFor example:\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n\n\n\nA section at the top of your _targets.R script where you define variables that you might tweak or modify can make it really nice when you are working with {targets}. You can use it to define, for example:\n\npaths to external files\nthemes for {ggplot2} figures\noptions and columns names to pass to functions\ngrouping columns e.g., id or species\n\nHere’s a couple different example projects that use a Variables section in their _targets.R scripts:\n\nrobitalec/targets-spatsoc-networks/_targets.R\nrobitalec/targets-issa/_targets.R\nrobitalec/targets-rgee-extract/_targets.R\n\n\n\nInstruction: make a new section named Variables, and add the file paths for the input files that we have been working with.\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n\n\n\nNow that our _targets.R script is setup, let’s add our targets. As mentioned above, targets are combined in a list at the end of the _targets.R script. The function to specify an individual target is tar_targets() and it has two main arguments we will focus on for now: name and command.\ntar_target(name, command)\n\n\nThe name of the target is the first argument of tar_target(). It is used in subsequent targets to connect dependencies between commands.\nFor example:\nc(\n    tar_target(target_1, command_1()),\n    tar_target(target_2, command_2(target_1))\n)\nNote: a target’s name cannot match the name of any objects in the global environment, including function names.\nOften, it feels natural to name a target using the name of the function we pass to command (tar_target(plot, plot(mtcars))). This is not an option - a target’s name shouldn’t match anything in the global environment (including function names). Treat the target name as if it was a variable - you wouldn’t want to assign your function to the name “plot” and a variable to the name “plot”. You can name based on a modification of the function name, eg. taking the noun_verb() syntax and making it past tense (name: plotted_... and command: plot_...), or a more specific name that refers to the function and the data (name: plot_forest and command plot(forest)).\n\n\n\nEach target in a pipeline is a step in analysis. See recommendations on what a target should do and how much a target should do in the {targets} manual.\nIn our case, we are going to plug in the functions we developed in the previous exercises into our target’s commands. Commands can also point to a function from a package, or to a multiline statement surrounded in curly braces.\nFor example:\n_targets.R\nc(\n    # A function from another package\n    tar_target(\n        car_model,\n        lme4::glmer(mtcars)\n    ),\n    # A multiline statement with {}\n    tar_target(\n        long_statement,\n        {\n            mpg &lt;- mtcars$mpg\n            cyl &lt;- mtcars$cyl\n            mpg / cyl\n        }\n    )\n)\n\n\n\nInstruction: set up individual targets corresponding to the three functions we developed using the adult and chick counts dataset. Head back to your testing scripts to recall how you used the functions and which arguments you need. Name your targets then pass the functions to the command argument. Fill in your Variables section and pass them as arguments in the functions.\n\nprepare_csv()\nsum_counts()\nplot_xy()\n\n\n\nHint: setup\n\nLoad targets\nSource the R/ directory\nSave the path to the counts data under Variables\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n\n)\n\n\n\nHint: prepare_csv()\n\nAdd a target for the prepare_csv() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the path_counts variable to the prepare_csv() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    )\n)\n\n\n\nHint: sum_counts()\n\nAdd a target for the sum_counts() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the target for the prepare_csv() function to the sum_counts() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    ),\n    tar_target(\n        sums,\n        sum_counts(prep_counts)\n    )\n)\n\n\n\nHint: plot_xy()\n\nAdd a target for the plot_xy() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the target for the prepare_csv() function to the plot_xy() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\nx_col &lt;- 'date_gmt'\ny_col &lt;- 'adults'\n\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    ),\n    tar_target(\n        sums,\n        sum_counts(prep_counts)\n    ),\n    tar_target(\n        plotted,\n        plot_xy(prep_counts, x_col, y_col)\n    )\n)\n\n\n\n\nadd two more targets to read the other CSV files\nadd another target to load the palmerpenguins dataset from {palmerpenguins}"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#targets.r",
    "href": "exercises/targets/01-targets-setup.html#targets.r",
    "title": "Setup",
    "section": "",
    "text": "The _targets.R is where we declare our {targets} pipeline. It is an R file saved in the root directory of your project. The generalized structure of a _targets.R file is:\n\nLoad packages\nSet options and variables\nEnd in a list of targets\n\nNote: our _targets.R script must end in a list of targets.\nA simple example script could be:\n_targets.R\nlibrary(targets)\n\ndata(mtcars)\n\nc(\n    tar_target(\n        histogram_mpg, \n        hist(mtcars$mpg)\n    )\n)\n\n\nInstruction: make a new R script in the root directory of your project named _targets.R. At the top, load {targets} and at the bottom put an empty list c( )."
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#packages",
    "href": "exercises/targets/01-targets-setup.html#packages",
    "title": "Setup",
    "section": "",
    "text": "To declare which packages we need across all our functions, we have two main options:\n\nList them with library() calls at the top of our _targets.R script\nList them all in a R/packages.R script and source that script.\n\nWe prefer the second approach because it keeps our _targets.R tidier, and helps us setup {renv} package versioning and {conflicted} later. You might find your list of packages gets long, and this gets them out of our way. You can also use this script when you are exploring your results later - source the packages.R script and you will have all your required packages.\n\n\nInstruction: make a new script called packages.R in the R/ directory. Go through your function test scripts from previous exercises and add all the packages you used to the packages.R file with a library() call for each package on a separate line.\nFor example:\nR/packages.R\nlibrary(targets)\nlibrary(data.table)\nlibrary(ggplot2)\nNow, to make those packages available to the {targets} pipeline, we need to load them in our _targets.R script.\nInstruction: add a command to source the R/packages.R in your _targets.R script.\nFor example:\n_targets.R\n# Packages\nsource(file.path('R', 'packages.R'))"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#functions",
    "href": "exercises/targets/01-targets-setup.html#functions",
    "title": "Setup",
    "section": "",
    "text": "To make functions available to the {targets} pipeline we also need to also source them in the _targets.R script.\nA shortcut for sourcing all the R files in a specific folder is provided by {targets}: tar_source(). We will use this to source our packages and functions in one step.\nNote: something to be careful about now that we are sourcing the R/ directory - you shouldn’t have any calls to {targets} functions like tar_read(), tar_load() or tar_make() in the script in the R/ directory. This is an impossible circular dependency - you are load or reading a target while you are also in the process of making it. Put any tar_read(), tar_load() or tar_make() calls elsewhere.\n\n\nInstruction: replace the above source('R/packages.R') command with the function tar_source() pointing to the R/ directory with your packages.R script and the functions you developed in the previous exercises.\nNote: you will need to load {targets} to run the tar_source() function.\nFor example:\n_targets.R\nlibrary(targets)\ntar_source('R')"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#options",
    "href": "exercises/targets/01-targets-setup.html#options",
    "title": "Setup",
    "section": "",
    "text": "We’ll come back to setting {targets} options later, but let’s set up a spot in our _targets.R script reserved for options. The relevant {targets} function is tar_option_set(). It can set global options for each target, such as the file format to save them in and when they should be cued to rerun. In addition, you can use tar_option_set() to manage options related to overall pipeline runs such as what to do when a targets hits an error.\nThe full reference page for tar_option_set() is available here: https://docs.ropensci.org/targets/reference/tar_option_set.html\n\n\nInstruction: make a new section in your _targets.R script named Options, and put an empty call to tar_option_set() there.\nFor example:\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#variables",
    "href": "exercises/targets/01-targets-setup.html#variables",
    "title": "Setup",
    "section": "",
    "text": "A section at the top of your _targets.R script where you define variables that you might tweak or modify can make it really nice when you are working with {targets}. You can use it to define, for example:\n\npaths to external files\nthemes for {ggplot2} figures\noptions and columns names to pass to functions\ngrouping columns e.g., id or species\n\nHere’s a couple different example projects that use a Variables section in their _targets.R scripts:\n\nrobitalec/targets-spatsoc-networks/_targets.R\nrobitalec/targets-issa/_targets.R\nrobitalec/targets-rgee-extract/_targets.R\n\n\n\nInstruction: make a new section named Variables, and add the file paths for the input files that we have been working with.\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')"
  },
  {
    "objectID": "exercises/targets/01-targets-setup.html#targets",
    "href": "exercises/targets/01-targets-setup.html#targets",
    "title": "Setup",
    "section": "",
    "text": "Now that our _targets.R script is setup, let’s add our targets. As mentioned above, targets are combined in a list at the end of the _targets.R script. The function to specify an individual target is tar_targets() and it has two main arguments we will focus on for now: name and command.\ntar_target(name, command)\n\n\nThe name of the target is the first argument of tar_target(). It is used in subsequent targets to connect dependencies between commands.\nFor example:\nc(\n    tar_target(target_1, command_1()),\n    tar_target(target_2, command_2(target_1))\n)\nNote: a target’s name cannot match the name of any objects in the global environment, including function names.\nOften, it feels natural to name a target using the name of the function we pass to command (tar_target(plot, plot(mtcars))). This is not an option - a target’s name shouldn’t match anything in the global environment (including function names). Treat the target name as if it was a variable - you wouldn’t want to assign your function to the name “plot” and a variable to the name “plot”. You can name based on a modification of the function name, eg. taking the noun_verb() syntax and making it past tense (name: plotted_... and command: plot_...), or a more specific name that refers to the function and the data (name: plot_forest and command plot(forest)).\n\n\n\nEach target in a pipeline is a step in analysis. See recommendations on what a target should do and how much a target should do in the {targets} manual.\nIn our case, we are going to plug in the functions we developed in the previous exercises into our target’s commands. Commands can also point to a function from a package, or to a multiline statement surrounded in curly braces.\nFor example:\n_targets.R\nc(\n    # A function from another package\n    tar_target(\n        car_model,\n        lme4::glmer(mtcars)\n    ),\n    # A multiline statement with {}\n    tar_target(\n        long_statement,\n        {\n            mpg &lt;- mtcars$mpg\n            cyl &lt;- mtcars$cyl\n            mpg / cyl\n        }\n    )\n)\n\n\n\nInstruction: set up individual targets corresponding to the three functions we developed using the adult and chick counts dataset. Head back to your testing scripts to recall how you used the functions and which arguments you need. Name your targets then pass the functions to the command argument. Fill in your Variables section and pass them as arguments in the functions.\n\nprepare_csv()\nsum_counts()\nplot_xy()\n\n\n\nHint: setup\n\nLoad targets\nSource the R/ directory\nSave the path to the counts data under Variables\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n\n)\n\n\n\nHint: prepare_csv()\n\nAdd a target for the prepare_csv() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the path_counts variable to the prepare_csv() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    )\n)\n\n\n\nHint: sum_counts()\n\nAdd a target for the sum_counts() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the target for the prepare_csv() function to the sum_counts() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    ),\n    tar_target(\n        sums,\n        sum_counts(prep_counts)\n    )\n)\n\n\n\nHint: plot_xy()\n\nAdd a target for the plot_xy() step\nGive it a meaningful name that isn’t the same as another object in your environment\nPass the target for the prepare_csv() function to the plot_xy() function\n_targets.R\nlibrary(targets)\ntar_source('R')\n\n# Options\ntar_option_set()\n\n# Variables\npath_counts &lt;- file.path('raw-data', 'adelie-adult-chick-counts.csv')\n\nx_col &lt;- 'date_gmt'\ny_col &lt;- 'adults'\n\n\n# Targets\nc(\n    tar_target(\n        prep_counts,\n        prepare_csv(path_counts)\n    ),\n    tar_target(\n        sums,\n        sum_counts(prep_counts)\n    ),\n    tar_target(\n        plotted,\n        plot_xy(prep_counts, x_col, y_col)\n    )\n)\n\n\n\n\nadd two more targets to read the other CSV files\nadd another target to load the palmerpenguins dataset from {palmerpenguins}"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html",
    "href": "exercises/targets/02-targets-usage.html",
    "title": "Usage",
    "section": "",
    "text": "Now that we have our _targets.R script setup with the previous exercises and we have added our first few targets to the pipeline, let’s take a look at using {targets}. It is a bit different than the usual R approach where we make edits, then rerun everything, wait for results, notice issues, rerun from scratch…\nThe following functions allow us to interact with the pipeline’s definition, metadata and results. Because of this, we shouldn’t put them inside the _targets.R script or in our R/ directory. Often a {targets} pipeline will result in a rendered document or manuscript, a number of saved plots or an output data file.\nIn our case, let’s make a new script called explore.R in the root of the project’s directory. Then we can keep track of our commands like tar_meta(), tar_visnetwork() and tar_read(), and easily rerun them when we make changes to the pipeline.\nInstruction: make a empty script called explore.R in the root of the project’s directory. At the top, load the packages and/or the functions with source() or tar_source().\nexplore.R\nlibrary(targets)\ntar_source('R')\n\n\nBefore we run our pipeline, we can check that everything looks good in the dependency graph with tar_visnetwork(). By default, tar_visnetwork() shows objects, functions and targets. You can adjust this behaviour to only show the targets using the argument targets_only set to TRUE.\nAnother argument that is especially useful when you have many targets: name. To show only selected targets, you can use {tidyselect} helpers like starts_with(). The name argument comes up in many other {targets} functions including tar_meta() and tar_read().\nRelated functions to tar_visnetwork() include:\n\ntar_glimpse() is a faster version of tar_visnetwork() that doesn’t check the pipeline’s metadata to see if functions have been are outdated or not\ntar_mermaid() returns a Mermaid.js diagram representing the pipeline\ntar_network() returns a network of edges and nodes representing the pipeline\n\n\n\nInstruction: run tar_visnetwork() from your explore.R script to see the dependency graph defined in your _targets.R script. Try out the targets_only and name arguments too.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_visnetwork()\n\ntar_visnetwork(targets_only = TRUE)\n\ntar_visnetwork(starts_with('plot'))\n\n\n\n\n\nRun the {targets} pipeline with tar_make(). The pipeline is run in a new external R process, which means that tar_make() doesn’t impact your current workspace and, more importantly, is not influenced by your current workspace. This totally isolated environment is how {targets} offers us a truly reproducible approach - only things defined in the _targets.R script (or sourced within it) are considered.\n\n\nInstruction: run tar_make() from your explore.R script.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_make()\n\n\n\n\n\nTo look at results from the {targets} pipeline, use tar_read() and tar_load(). tar_read() reads targets without saving them as an object in your environment and tar_load() loads the object directly to your environment using the name of the target as the object.\n\n\nInstruction: run tar_read() and tar_load() from your explore.R script to read/load the targets defined in your {targets} pipeline.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_read(prep_counts)\n\ntar_load(sums)\n\ntar_read(plotted)\n\n\n\n\n\nThe metadata related to the {targets} pipeline is accessible using the function tar_meta().\n\n\nInstruction: run tar_meta() from your explore.R script to look at the metadata associated with your {targets} pipeline. Also try using the names argument to filter the metadata returned.\nNote: we sometimes find it helpful to use the View() function to open the metadata as a table in RStudio.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_meta()\n\ntar_meta(starts_with('plot'))\n\nView(tar_meta())\n\n\n\n\n\nNow that we have a {targets} pipeline, the approach is a bit different than we might be used to using R interactively. With a {targets} pipeline, everything is defined in the pipeline so our process is now about making edits to our functions and our _targets.R pipeline, then rerunning tar_make(). {targets} tracks all functions and objects (and optionally can also track external files) and will only rerun downstream targets when required. This is the other main way that {targets} makes our lives easier. We no longer need to mentally keep track of versions of scripts, output files, figures, etc - just run tar_make() to run anything outdated (or check with tar_visnetwork()).\n\n\nInstruction: make sure your {targets} pipeline is up to date with tar_visnetwork(). Then, make an edit to the function sum_counts(). Rerun the {targets} pipeline. Next, make an edit to the function plot_xy(). Rerun with tar_make(). Lastly, make an edit to prepare_csv() and rerun. Note which targets were rerun with each change and discuss with your neighbours.\nBonus:\n\nmake a change to prepare_csv() that doesn’t actually change the returned object (eg. by adding an irrelevant intermediate step). Does this force the {targets} pipeline to rerun?\n\n\n\n\n\nThe following functions can be used to force {targets} to rerun targets.\n\ntar_delete() deletes individual target output values\ntar_destroy() deletes the whole _targets/ data store (caution)\n\n\n\n\nMake more targets!\n\nuse the plot_xy() function with the weather timeseries\nuse the plot_xy() function with the palmerpenguins timeseries\nmake a target for something that isn’t one of our custom functions eg. summary(). ({targets} don’t need to be custom functions, any R command can be used)"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#tar_visnetwork",
    "href": "exercises/targets/02-targets-usage.html#tar_visnetwork",
    "title": "Usage",
    "section": "",
    "text": "Before we run our pipeline, we can check that everything looks good in the dependency graph with tar_visnetwork(). By default, tar_visnetwork() shows objects, functions and targets. You can adjust this behaviour to only show the targets using the argument targets_only set to TRUE.\nAnother argument that is especially useful when you have many targets: name. To show only selected targets, you can use {tidyselect} helpers like starts_with(). The name argument comes up in many other {targets} functions including tar_meta() and tar_read().\nRelated functions to tar_visnetwork() include:\n\ntar_glimpse() is a faster version of tar_visnetwork() that doesn’t check the pipeline’s metadata to see if functions have been are outdated or not\ntar_mermaid() returns a Mermaid.js diagram representing the pipeline\ntar_network() returns a network of edges and nodes representing the pipeline\n\n\n\nInstruction: run tar_visnetwork() from your explore.R script to see the dependency graph defined in your _targets.R script. Try out the targets_only and name arguments too.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_visnetwork()\n\ntar_visnetwork(targets_only = TRUE)\n\ntar_visnetwork(starts_with('plot'))"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#tar_make",
    "href": "exercises/targets/02-targets-usage.html#tar_make",
    "title": "Usage",
    "section": "",
    "text": "Run the {targets} pipeline with tar_make(). The pipeline is run in a new external R process, which means that tar_make() doesn’t impact your current workspace and, more importantly, is not influenced by your current workspace. This totally isolated environment is how {targets} offers us a truly reproducible approach - only things defined in the _targets.R script (or sourced within it) are considered.\n\n\nInstruction: run tar_make() from your explore.R script.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_make()"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#tar_read-tar_load",
    "href": "exercises/targets/02-targets-usage.html#tar_read-tar_load",
    "title": "Usage",
    "section": "",
    "text": "To look at results from the {targets} pipeline, use tar_read() and tar_load(). tar_read() reads targets without saving them as an object in your environment and tar_load() loads the object directly to your environment using the name of the target as the object.\n\n\nInstruction: run tar_read() and tar_load() from your explore.R script to read/load the targets defined in your {targets} pipeline.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_read(prep_counts)\n\ntar_load(sums)\n\ntar_read(plotted)"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#tar_meta",
    "href": "exercises/targets/02-targets-usage.html#tar_meta",
    "title": "Usage",
    "section": "",
    "text": "The metadata related to the {targets} pipeline is accessible using the function tar_meta().\n\n\nInstruction: run tar_meta() from your explore.R script to look at the metadata associated with your {targets} pipeline. Also try using the names argument to filter the metadata returned.\nNote: we sometimes find it helpful to use the View() function to open the metadata as a table in RStudio.\n\n\nHint\n\nexplore.R\nlibrary(targets)\ntar_source('R')\n\ntar_meta()\n\ntar_meta(starts_with('plot'))\n\nView(tar_meta())"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#edits-rerun-edits",
    "href": "exercises/targets/02-targets-usage.html#edits-rerun-edits",
    "title": "Usage",
    "section": "",
    "text": "Now that we have a {targets} pipeline, the approach is a bit different than we might be used to using R interactively. With a {targets} pipeline, everything is defined in the pipeline so our process is now about making edits to our functions and our _targets.R pipeline, then rerunning tar_make(). {targets} tracks all functions and objects (and optionally can also track external files) and will only rerun downstream targets when required. This is the other main way that {targets} makes our lives easier. We no longer need to mentally keep track of versions of scripts, output files, figures, etc - just run tar_make() to run anything outdated (or check with tar_visnetwork()).\n\n\nInstruction: make sure your {targets} pipeline is up to date with tar_visnetwork(). Then, make an edit to the function sum_counts(). Rerun the {targets} pipeline. Next, make an edit to the function plot_xy(). Rerun with tar_make(). Lastly, make an edit to prepare_csv() and rerun. Note which targets were rerun with each change and discuss with your neighbours.\nBonus:\n\nmake a change to prepare_csv() that doesn’t actually change the returned object (eg. by adding an irrelevant intermediate step). Does this force the {targets} pipeline to rerun?"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#forcing-reruns",
    "href": "exercises/targets/02-targets-usage.html#forcing-reruns",
    "title": "Usage",
    "section": "",
    "text": "The following functions can be used to force {targets} to rerun targets.\n\ntar_delete() deletes individual target output values\ntar_destroy() deletes the whole _targets/ data store (caution)"
  },
  {
    "objectID": "exercises/targets/02-targets-usage.html#bonus",
    "href": "exercises/targets/02-targets-usage.html#bonus",
    "title": "Usage",
    "section": "",
    "text": "Make more targets!\n\nuse the plot_xy() function with the weather timeseries\nuse the plot_xy() function with the palmerpenguins timeseries\nmake a target for something that isn’t one of our custom functions eg. summary(). ({targets} don’t need to be custom functions, any R command can be used)"
  },
  {
    "objectID": "exercises/targets/03-targets-extensions.html",
    "href": "exercises/targets/03-targets-extensions.html",
    "title": "Extensions",
    "section": "",
    "text": "Let’s look at a {targets} pipeline that more closely resembles what a full project would be including figures, model, output, tables and a manuscript object. We’ll start with a demo, so we can show off some of the power and extended features of {targets}, then we can work through it piece by piece to explain the details.\nLater, check out some examples of full {targets} pipelines that we’ve developed to get a sense for the various applications and approaches that can be used.\n\ndata cleaning and preparation pipeline: https://github.com/robitalec/prepare-locs/blob/main/_targets.R\nmovement ecology method: https://github.com/robitalec/targets-issa/blob/main/_targets.R\nremote sensing / spatial application: https://github.com/robitalec/targets-rgee-extract/blob/main/_targets.R\nBayesian models with {brms} - https://github.com/robitalec/statistical-rethinking-colearning-2023/blob/main/_targets.R\n\n\n\nWith our extended {targets} pipeline, we’ll go through the following steps:\n\nVisualize the pipeline’s steps and interdependencies.\n\ntar_visnetwork()\n\nRun the workflow\n\ntar_make()\n\nExplore the results\n\nlibrary(fs)\ndir_tree('figures')\nfigures\n├── CHR.png\n├── COR.png\n├── HUM.png\n├── LIT.png\n└── TOR.png\ndir_tree('output')\noutput\n├── sums.csv\n\nLook at the rendered manuscript document\n\nOpen the rendered document in the paper/ directory.\n\nNow to make some changes\n\nEdit the raw-data file, for example by adding new rows to mimic more data being collected. Note: please do not edit raw data, this is just to illustrate how {targets} tracks external files for changes. Rerun tar_make().\nDelete an output figure. Rerun tar_make().\nAdd a filter step before the group counts target. Rerun tar_make().\n\n\n\n\n\nObviously, this is an overwhelming amount of new {targets} functions and extensions combined with other packages and Quarto to render an example manuscript and output figures and tables. We’ll walk through each of the pieces and how they relate to each other, and we hope this is a useful resource for when you bring {targets} to your next project.\n\n\nLike we mentioned in the functions resources, there are plenty of options in R for iterating including:\n\nthe apply family\n{purrr}\n{dplyr} with group_by and mutate\n{data.table}’s by\n\n{targets} offers a new approach that integrates into our pipeline seamlessly and extends our functions to run over groups of rows in a data.frame. There are two steps:\n\nUse the {tarchetypes} function tar_group_by() to define the variables to group the data.frame on.\nUse “dynamic branching” with the tar_target() argument pattern.\n\nFor example:\n_targets.R\n#...\nc(\n    # Group the mtcars data.frame by the values in the column \"cyl\"\n    tar_group_by(\n        group_counts,\n        mtcars,\n        cyl\n    ),\n\n    # Averages by group\n    tar_target(\n        mean_mpg,\n        mean(mtcars$mpg),\n        pattern = map(mtcars)\n    ),\n\n    # Summarize all the cars\n    tar_target(\n        summarize_cars,\n        summarize(group_counts)\n    )\n)\nWe pass the data.frame to the command argument, and the following arguments are the column names to group on (in this case cyl). Then downstream targets that want to iterate over the groups of rows use the argument pattern with the function map(). The target corresponding to the grouped data.frame is passed to map().\nIf you want to use the full dataset, ungrouped, again downstream simply refer to the target without the pattern argument and {targets} will automatically combine the data.frame again.\nAnother approach that Alec likes to use with {data.table} is:\n_targets.R\n# ...\n\n# Variables\n# Split by: within which column or set of columns (eg. c(id, yr))\n#  do we want to split our analysis?\nsplit_by &lt;- c('id', 'species')\n\n# Targets\nc(\n    tar_target(\n        split_data,\n        data[, tar_group := .GRP, by = split_by],\n        iteration = 'group'\n    ),\n    tar_target(\n        split_key,\n        unique(locs_prep[, .SD, .SDcols = c(split_by, 'tar_group')])\n    )\n\n)\nRead more:\n\ntar_group_by(): https://docs.ropensci.org/tarchetypes/reference/tar_group_by.html\ntar_group_count(): https://docs.ropensci.org/tarchetypes/reference/tar_group_count.html\ntar_group(): https://docs.ropensci.org/targets/reference/tar_group.html\n\nThese are example of “dynamic branching”, but also see “static branching” functions:\n\ntar_map: https://docs.ropensci.org/tarchetypes/reference/tar_map.html\ntar_rep: https://docs.ropensci.org/tarchetypes/reference/tar_rep.html\n\n\n\nThe iteration argument for tar_target() is used to change how {targets} splits and combines branches. This is is required when you want to return objects like plots, models or matrices that aren’t obviously combined.\nFor example:\n_targets.R\n#...\nc(\n    # Group the mtcars data.frame by the values in the column \"cyl\"\n    tar_group_by(\n        group_counts,\n        mtcars,\n        cyl\n    ),\n\n    # Averages by group\n    tar_target(\n        plot_cars,\n        ggplot(mtcars) + geom_histogram(mpg),\n        pattern = map(mtcars),\n        iteration = 'list'\n    ),\n\n    # Matrix of correlations between vars\n    tar_target(\n        correlation_metrics,\n        cor(mtcars),\n        pattern = map(mtcars),\n        iteration = 'list'\n    )\n)\nSee more details for argument iteration in ?tar_target: https://docs.ropensci.org/targets/reference/tar_target.html\n\n\n\n\n{targets} can be used to track external files for changes. This includes input data files but also outputs like saved plots or tables. To signal to {targets} that these target represents an external file to track, use the argument ‘format’ set to ‘file’.\n_targets.R\n# ...\n\nc(\n    tar_target(\n        file_counts,\n        path_counts,\n        format = 'file'\n    ),\n# ...\n)\n\n\n\n\nStart your next project with {targets} because it is easier to start from a simple project then to try and restructure an old, large project.\nIf you use Git, you will likely want to ignore the _targets/ directory.\n\n\n\nLook into the arguments provided by tar_option_set(). This is link to the full reference page: https://docs.ropensci.org/targets/reference/tar_option_set.html\n\nformat argument\n\nfor data.table objects you can use {qs} and format qs to retain data.table clas\n\nerror argument to define what {targets} should do when it hits an error\nworkspace_on_error argument to optionally save a workspace file for each target that throws an error\ncue argument\n\nIf you are already done all the previous exercises and have extra time, or on your own after the workshop, try this bonus exercise:\n\nwrite a function that processes the ice data\nlook at the relationship between adult and chick counts, and the ice data and weather data. How might these two be influencing adult and chick counts?"
  },
  {
    "objectID": "exercises/targets/03-targets-extensions.html#demo",
    "href": "exercises/targets/03-targets-extensions.html#demo",
    "title": "Extensions",
    "section": "",
    "text": "With our extended {targets} pipeline, we’ll go through the following steps:\n\nVisualize the pipeline’s steps and interdependencies.\n\ntar_visnetwork()\n\nRun the workflow\n\ntar_make()\n\nExplore the results\n\nlibrary(fs)\ndir_tree('figures')\nfigures\n├── CHR.png\n├── COR.png\n├── HUM.png\n├── LIT.png\n└── TOR.png\ndir_tree('output')\noutput\n├── sums.csv\n\nLook at the rendered manuscript document\n\nOpen the rendered document in the paper/ directory.\n\nNow to make some changes\n\nEdit the raw-data file, for example by adding new rows to mimic more data being collected. Note: please do not edit raw data, this is just to illustrate how {targets} tracks external files for changes. Rerun tar_make().\nDelete an output figure. Rerun tar_make().\nAdd a filter step before the group counts target. Rerun tar_make()."
  },
  {
    "objectID": "exercises/targets/03-targets-extensions.html#details",
    "href": "exercises/targets/03-targets-extensions.html#details",
    "title": "Extensions",
    "section": "",
    "text": "Obviously, this is an overwhelming amount of new {targets} functions and extensions combined with other packages and Quarto to render an example manuscript and output figures and tables. We’ll walk through each of the pieces and how they relate to each other, and we hope this is a useful resource for when you bring {targets} to your next project.\n\n\nLike we mentioned in the functions resources, there are plenty of options in R for iterating including:\n\nthe apply family\n{purrr}\n{dplyr} with group_by and mutate\n{data.table}’s by\n\n{targets} offers a new approach that integrates into our pipeline seamlessly and extends our functions to run over groups of rows in a data.frame. There are two steps:\n\nUse the {tarchetypes} function tar_group_by() to define the variables to group the data.frame on.\nUse “dynamic branching” with the tar_target() argument pattern.\n\nFor example:\n_targets.R\n#...\nc(\n    # Group the mtcars data.frame by the values in the column \"cyl\"\n    tar_group_by(\n        group_counts,\n        mtcars,\n        cyl\n    ),\n\n    # Averages by group\n    tar_target(\n        mean_mpg,\n        mean(mtcars$mpg),\n        pattern = map(mtcars)\n    ),\n\n    # Summarize all the cars\n    tar_target(\n        summarize_cars,\n        summarize(group_counts)\n    )\n)\nWe pass the data.frame to the command argument, and the following arguments are the column names to group on (in this case cyl). Then downstream targets that want to iterate over the groups of rows use the argument pattern with the function map(). The target corresponding to the grouped data.frame is passed to map().\nIf you want to use the full dataset, ungrouped, again downstream simply refer to the target without the pattern argument and {targets} will automatically combine the data.frame again.\nAnother approach that Alec likes to use with {data.table} is:\n_targets.R\n# ...\n\n# Variables\n# Split by: within which column or set of columns (eg. c(id, yr))\n#  do we want to split our analysis?\nsplit_by &lt;- c('id', 'species')\n\n# Targets\nc(\n    tar_target(\n        split_data,\n        data[, tar_group := .GRP, by = split_by],\n        iteration = 'group'\n    ),\n    tar_target(\n        split_key,\n        unique(locs_prep[, .SD, .SDcols = c(split_by, 'tar_group')])\n    )\n\n)\nRead more:\n\ntar_group_by(): https://docs.ropensci.org/tarchetypes/reference/tar_group_by.html\ntar_group_count(): https://docs.ropensci.org/tarchetypes/reference/tar_group_count.html\ntar_group(): https://docs.ropensci.org/targets/reference/tar_group.html\n\nThese are example of “dynamic branching”, but also see “static branching” functions:\n\ntar_map: https://docs.ropensci.org/tarchetypes/reference/tar_map.html\ntar_rep: https://docs.ropensci.org/tarchetypes/reference/tar_rep.html\n\n\n\nThe iteration argument for tar_target() is used to change how {targets} splits and combines branches. This is is required when you want to return objects like plots, models or matrices that aren’t obviously combined.\nFor example:\n_targets.R\n#...\nc(\n    # Group the mtcars data.frame by the values in the column \"cyl\"\n    tar_group_by(\n        group_counts,\n        mtcars,\n        cyl\n    ),\n\n    # Averages by group\n    tar_target(\n        plot_cars,\n        ggplot(mtcars) + geom_histogram(mpg),\n        pattern = map(mtcars),\n        iteration = 'list'\n    ),\n\n    # Matrix of correlations between vars\n    tar_target(\n        correlation_metrics,\n        cor(mtcars),\n        pattern = map(mtcars),\n        iteration = 'list'\n    )\n)\nSee more details for argument iteration in ?tar_target: https://docs.ropensci.org/targets/reference/tar_target.html\n\n\n\n\n{targets} can be used to track external files for changes. This includes input data files but also outputs like saved plots or tables. To signal to {targets} that these target represents an external file to track, use the argument ‘format’ set to ‘file’.\n_targets.R\n# ...\n\nc(\n    tar_target(\n        file_counts,\n        path_counts,\n        format = 'file'\n    ),\n# ...\n)"
  },
  {
    "objectID": "exercises/targets/03-targets-extensions.html#recommendations",
    "href": "exercises/targets/03-targets-extensions.html#recommendations",
    "title": "Extensions",
    "section": "",
    "text": "Start your next project with {targets} because it is easier to start from a simple project then to try and restructure an old, large project.\nIf you use Git, you will likely want to ignore the _targets/ directory."
  },
  {
    "objectID": "exercises/targets/03-targets-extensions.html#bonus",
    "href": "exercises/targets/03-targets-extensions.html#bonus",
    "title": "Extensions",
    "section": "",
    "text": "Look into the arguments provided by tar_option_set(). This is link to the full reference page: https://docs.ropensci.org/targets/reference/tar_option_set.html\n\nformat argument\n\nfor data.table objects you can use {qs} and format qs to retain data.table clas\n\nerror argument to define what {targets} should do when it hits an error\nworkspace_on_error argument to optionally save a workspace file for each target that throws an error\ncue argument\n\nIf you are already done all the previous exercises and have extra time, or on your own after the workshop, try this bonus exercise:\n\nwrite a function that processes the ice data\nlook at the relationship between adult and chick counts, and the ice data and weather data. How might these two be influencing adult and chick counts?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "Workshop at CSEE 2023: developing a reproducible workflow in R using functions, {targets} and {renv}\nDeveloped and delivered by:\n\nAlec L. Robitaille (Memorial University of Newfoundland and Labrador)\nIsabella C. Richmond (Concordia University)\n\n\n\nProjects\n\nDirectories\nREADMEs\nRStudio Projects\n\nShort break\nFunctions\n\nIntroduction\nRecommended approach\nChecks\nOptions\n\nLunch break\n{targets}\n\nIntroduction\nWriting workflows\nVisualizing\nRunning workflows\nExtensions\n\nShort break\n{renv} + {conflicted}\n\nSaving package versions\nChecking conflicts\n\n\n\n\nOverall\n\nApproach analyses in a more holistic way (whole project vs script by script)\nShare data across projects and software versions with minimal stress \nUse workflows that reduce analysis errors and mental load\n\nSection 1: Projects\n\nConstruct a RStudio project that is thoroughly documented using file structure and data management best practices\nUse RStudio projects to effectively share their own work, and use other people’s\n\nSection 2: Functions\n\nRead and understand structure of functions in R\nRefactor code into functions that do one thing\nAdd tests and checks to ensure functions work and error when expected\nRecognize the value of functions as chunks of code that are reusable and easier to debug\n\nSection 3: {targets}\n\n(For a given project) map out relationships between inputs, outputs and analysis steps\nIdentify discrete chunks/steps and write corresponding (or use available) functions \nExecute a workflow in {targets} that reads in data, performs a function, and saves an output\nRecognize the value of workflows for reducing mental load and improving efficiency\n\nSection 4: {renv} + {conflicted}\n\nUse {renv} to preserve current package versions to ensure the environment is reproducible, portable and isolated\nUse {conflicted} to detect conflicting function names\n\n\n\n\nThis workshop is aimed at improving our ability to use and create reproducible workflows. All the materials should be accessible from the side bar (slides, exercises, resources for further reading, and the link to the GitHub repository can be accessed by clicking on the GitHub icon).\nWe don’t have any strict dependencies on specific versions of R or R packages, but it would be good to have at least R version 4.0 and a recent version of RStudio. \nWe are using Quarto to build the workshop’s website and exercises, so it could be helpful for you to install it too. If you don’t have time to, you can always complete exercises in an R script - so no pressure. \nInstall first the Quarto CLI from the here then the package with the command at the bottom.\nPlease install the following packages (after updating R):\npkgs &lt;- c(\n  'targets',\n  'igraph',\n  'data.table',\n  'dplyr',\n  'ggplot2',\n  'testthat',\n  'janitor',\n  'renv',\n  'rlang',\n  'conflicted',\n  'palmerpenguins',\n  'visNetwork',\n  'quarto'\n)\n\ninstall.packages(pkgs)\n\n\n\nExample data for this workshop is borrowed from the Palmer Long-Term Ecological Research (LTER). Here is the study description from the Palmer LTER site:\n\nThe Palmer Long-Term Ecological Research (LTER) study area is located to the west of the Antarctic Peninsula extending South and North of the Palmer Basin from onshore to several hundred kilometers off shore. Palmer Station is one of the three United States research stations located in Antarctica. It is on Anvers Island midway down the Antarctic Peninsula at latitude 64.7 South, longitude 64.0 West.\n\n\nThe Palmer LTER studies a polar marine biome with research focused on the Antarctic pelagic marine ecosystem, including sea ice habitats, regional oceanography and terrestrial nesting sites of seabird predators. The Palmer LTER is one of more than 26 LTER research sites located throughout the United States, Puerto Rico and Tahiti; each focused on a specific ecosystem, that together constitute the LTER Network.\n\nWe gratefully acknowledge the Palmer LTER for releasing data freely and openly for diverse uses - in our case for training analytical skills of researchers in ecology.\n\n\nThe first dataset is already available in R through the palmerpenguins R package. There is a raw version and a cleaned version. They contain data for 344 penguins, with the following variables (cleaned version):\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nThe following datasets are available directly from the Palmer LTER Data Catalog. To download the data to the raw-data/ directory, run the function download_example_data() (R/download_example_data.R).\nThis second dataset contains monthly averaged weather timeseries from Palmer Station, Antarctica, with the following variables:\n\nDate\n\nYear\n\nMonth\n\nAverage Temperature\n\nTemperature Count\n\nAverage Pressure\n\nPressure Count\n\nAverage Melted Precipitation\n\nPrecipitation Count\n\nData package summary\nData package metadata\nLink to data (CSV):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.189.8&entityid=ab357b4c92531a07d98ff1c4f4809a1e\n\n\n\nThe third dataset contains monthly sea ice area from the region around the Palmer Station, Antarctica, with the following variables:\n\nYear\nMonth\nArea\n\nNote: this data is formatted with months as columns, years as rows, and cells filled with the corresponding area.\nData package summary\nData package metadata\nLink to data (TXT):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.34.7&entityid=0fccb4e99aaa0c0cc85c23284288ec81\n\n\n\nThe fourth dataset contains Adelie penguin adult and chick counts\n\nstudyName\n\nDate GMT\n\nTime GMT\n\nIsland\n\nColony\n\nAdults\n\nChicks\n\nData package summary\nData package metadata\nLink to data (CSV):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.88.8&entityid=b4062890db09a72628786650dacfbf1f\n\n\n\n\nThis project is released under the GNU General Public License v3.0. Read it here."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "Projects\n\nDirectories\nREADMEs\nRStudio Projects\n\nShort break\nFunctions\n\nIntroduction\nRecommended approach\nChecks\nOptions\n\nLunch break\n{targets}\n\nIntroduction\nWriting workflows\nVisualizing\nRunning workflows\nExtensions\n\nShort break\n{renv} + {conflicted}\n\nSaving package versions\nChecking conflicts"
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "Overall\n\nApproach analyses in a more holistic way (whole project vs script by script)\nShare data across projects and software versions with minimal stress \nUse workflows that reduce analysis errors and mental load\n\nSection 1: Projects\n\nConstruct a RStudio project that is thoroughly documented using file structure and data management best practices\nUse RStudio projects to effectively share their own work, and use other people’s\n\nSection 2: Functions\n\nRead and understand structure of functions in R\nRefactor code into functions that do one thing\nAdd tests and checks to ensure functions work and error when expected\nRecognize the value of functions as chunks of code that are reusable and easier to debug\n\nSection 3: {targets}\n\n(For a given project) map out relationships between inputs, outputs and analysis steps\nIdentify discrete chunks/steps and write corresponding (or use available) functions \nExecute a workflow in {targets} that reads in data, performs a function, and saves an output\nRecognize the value of workflows for reducing mental load and improving efficiency\n\nSection 4: {renv} + {conflicted}\n\nUse {renv} to preserve current package versions to ensure the environment is reproducible, portable and isolated\nUse {conflicted} to detect conflicting function names"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "This workshop is aimed at improving our ability to use and create reproducible workflows. All the materials should be accessible from the side bar (slides, exercises, resources for further reading, and the link to the GitHub repository can be accessed by clicking on the GitHub icon).\nWe don’t have any strict dependencies on specific versions of R or R packages, but it would be good to have at least R version 4.0 and a recent version of RStudio. \nWe are using Quarto to build the workshop’s website and exercises, so it could be helpful for you to install it too. If you don’t have time to, you can always complete exercises in an R script - so no pressure. \nInstall first the Quarto CLI from the here then the package with the command at the bottom.\nPlease install the following packages (after updating R):\npkgs &lt;- c(\n  'targets',\n  'igraph',\n  'data.table',\n  'dplyr',\n  'ggplot2',\n  'testthat',\n  'janitor',\n  'renv',\n  'rlang',\n  'conflicted',\n  'palmerpenguins',\n  'visNetwork',\n  'quarto'\n)\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "Example data for this workshop is borrowed from the Palmer Long-Term Ecological Research (LTER). Here is the study description from the Palmer LTER site:\n\nThe Palmer Long-Term Ecological Research (LTER) study area is located to the west of the Antarctic Peninsula extending South and North of the Palmer Basin from onshore to several hundred kilometers off shore. Palmer Station is one of the three United States research stations located in Antarctica. It is on Anvers Island midway down the Antarctic Peninsula at latitude 64.7 South, longitude 64.0 West.\n\n\nThe Palmer LTER studies a polar marine biome with research focused on the Antarctic pelagic marine ecosystem, including sea ice habitats, regional oceanography and terrestrial nesting sites of seabird predators. The Palmer LTER is one of more than 26 LTER research sites located throughout the United States, Puerto Rico and Tahiti; each focused on a specific ecosystem, that together constitute the LTER Network.\n\nWe gratefully acknowledge the Palmer LTER for releasing data freely and openly for diverse uses - in our case for training analytical skills of researchers in ecology.\n\n\nThe first dataset is already available in R through the palmerpenguins R package. There is a raw version and a cleaned version. They contain data for 344 penguins, with the following variables (cleaned version):\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nThe following datasets are available directly from the Palmer LTER Data Catalog. To download the data to the raw-data/ directory, run the function download_example_data() (R/download_example_data.R).\nThis second dataset contains monthly averaged weather timeseries from Palmer Station, Antarctica, with the following variables:\n\nDate\n\nYear\n\nMonth\n\nAverage Temperature\n\nTemperature Count\n\nAverage Pressure\n\nPressure Count\n\nAverage Melted Precipitation\n\nPrecipitation Count\n\nData package summary\nData package metadata\nLink to data (CSV):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.189.8&entityid=ab357b4c92531a07d98ff1c4f4809a1e\n\n\n\nThe third dataset contains monthly sea ice area from the region around the Palmer Station, Antarctica, with the following variables:\n\nYear\nMonth\nArea\n\nNote: this data is formatted with months as columns, years as rows, and cells filled with the corresponding area.\nData package summary\nData package metadata\nLink to data (TXT):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.34.7&entityid=0fccb4e99aaa0c0cc85c23284288ec81\n\n\n\nThe fourth dataset contains Adelie penguin adult and chick counts\n\nstudyName\n\nDate GMT\n\nTime GMT\n\nIsland\n\nColony\n\nAdults\n\nChicks\n\nData package summary\nData package metadata\nLink to data (CSV):\nhttps://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.88.8&entityid=b4062890db09a72628786650dacfbf1f"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducible workflows workshop",
    "section": "",
    "text": "This project is released under the GNU General Public License v3.0. Read it here."
  },
  {
    "objectID": "resources/01-projects.html",
    "href": "resources/01-projects.html",
    "title": "Projects",
    "section": "",
    "text": "Good file structure allows you to manage all the components of your (often large) projects, while facilitating easy sharing and reducing the risk of accidentally deleting/altering important files. Keeping your raw data file in its own folder (e.g., input/ or raw/ makes it harder to mix up these files with intermediate ones down the line).\nSoftware Carpentry’s R for Reproducible Scientific Analysis:\nBest practices for file structure/data management include:\n\nTreat raw data as read-only\nStore data cleaning scripts in a separate folder and create a second “read-only” data folder to hold the “cleaned” data sets\nTreat generated output as disposable\n\nEfficient R Programming suggests a sub-directory resembling something like below to keep things tidy:\n\n\nproject\n└───input/\n└───output/\n└───R/\n└───graphics/\n└───README.md\n\nproject\n└───data/\n    └───derived/\n    └───raw-data/\n└───R/\n└───script/\n└───graphics/\n└───README.md\n\n\nGood Enough Practices in Scientific Computing suggests similar file structure and data management practices.\n\n\n\n\nA README file can act as a type of metadata (see below): it facilitates people using your data, script, etc.\nThere are basic requirements from a README in order to make your work usable (highlighted in our Think/Pair/Share exercise)\n\nARDC Metadata Guide In order to use data, we need to know:\n\nhow the data is structured what it describes\nhow to read it (e.g. column headings and units)\nmethodological information such as instrument settings and calibrations, reagents used, or survey questions\nexactly what they are allowed to do with the data through rights metadata such as licensing\nhow to acknowledge the original creators by citing the data\n\nReproducible Quantitative Methods Metadata is required for open data, by making a data reuse plan we can ensure that our data is usable for other people, into the future.\nMetadata should warn users about problems/inconsistencies in the data and provide checks to make sure data is functioning properly (White et al., 2013)\nCornell University best practices provides a README template that is free to adapt, alter, and use\nExamples:\n\nhttps://github.com/viralemergence/datacov\nhttps://github.com/wildlifeevoeco/MovingAcrossGradients\nhttps://github.com/ropensci/spatsoc\nhttps://github.com/robitalec/statistical-rethinking-colearning-2023\n\n\n\n\nUsing an RStudio Project makes sharing your data/code with others (and your future self) SO MUCH EASIER! One of the main issues with sharing code is the changing working directories/missing files/etc. The RStudio Project completely solves this for you. You can just copy and paste the folder wherever you need it, with nothing breaking.\nSoftware Carpentry’s R for Reproducible Scientific Analysis and Efficient R Programming both discuss further the importance of using RStudio Projects and how to set them up."
  },
  {
    "objectID": "resources/01-projects.html#file-structure",
    "href": "resources/01-projects.html#file-structure",
    "title": "Projects",
    "section": "",
    "text": "Good file structure allows you to manage all the components of your (often large) projects, while facilitating easy sharing and reducing the risk of accidentally deleting/altering important files. Keeping your raw data file in its own folder (e.g., input/ or raw/ makes it harder to mix up these files with intermediate ones down the line).\nSoftware Carpentry’s R for Reproducible Scientific Analysis:\nBest practices for file structure/data management include:\n\nTreat raw data as read-only\nStore data cleaning scripts in a separate folder and create a second “read-only” data folder to hold the “cleaned” data sets\nTreat generated output as disposable\n\nEfficient R Programming suggests a sub-directory resembling something like below to keep things tidy:\n\n\nproject\n└───input/\n└───output/\n└───R/\n└───graphics/\n└───README.md\n\nproject\n└───data/\n    └───derived/\n    └───raw-data/\n└───R/\n└───script/\n└───graphics/\n└───README.md\n\n\nGood Enough Practices in Scientific Computing suggests similar file structure and data management practices."
  },
  {
    "objectID": "resources/01-projects.html#readme",
    "href": "resources/01-projects.html#readme",
    "title": "Projects",
    "section": "",
    "text": "A README file can act as a type of metadata (see below): it facilitates people using your data, script, etc.\nThere are basic requirements from a README in order to make your work usable (highlighted in our Think/Pair/Share exercise)\n\nARDC Metadata Guide In order to use data, we need to know:\n\nhow the data is structured what it describes\nhow to read it (e.g. column headings and units)\nmethodological information such as instrument settings and calibrations, reagents used, or survey questions\nexactly what they are allowed to do with the data through rights metadata such as licensing\nhow to acknowledge the original creators by citing the data\n\nReproducible Quantitative Methods Metadata is required for open data, by making a data reuse plan we can ensure that our data is usable for other people, into the future.\nMetadata should warn users about problems/inconsistencies in the data and provide checks to make sure data is functioning properly (White et al., 2013)\nCornell University best practices provides a README template that is free to adapt, alter, and use\nExamples:\n\nhttps://github.com/viralemergence/datacov\nhttps://github.com/wildlifeevoeco/MovingAcrossGradients\nhttps://github.com/ropensci/spatsoc\nhttps://github.com/robitalec/statistical-rethinking-colearning-2023"
  },
  {
    "objectID": "resources/01-projects.html#rstudio-projects",
    "href": "resources/01-projects.html#rstudio-projects",
    "title": "Projects",
    "section": "",
    "text": "Using an RStudio Project makes sharing your data/code with others (and your future self) SO MUCH EASIER! One of the main issues with sharing code is the changing working directories/missing files/etc. The RStudio Project completely solves this for you. You can just copy and paste the folder wherever you need it, with nothing breaking.\nSoftware Carpentry’s R for Reproducible Scientific Analysis and Efficient R Programming both discuss further the importance of using RStudio Projects and how to set them up."
  },
  {
    "objectID": "resources/02-functions.html",
    "href": "resources/02-functions.html",
    "title": "Functions",
    "section": "",
    "text": "Links:\n\n1st edition\n2nd edition\nGitHub repository\n\n\n\nWhy write functions?\n\n“Improving reach as a data scientist”\n“Automate common tasks”\nThree big advantages\n\nGive a function an evocative name that makes your code easier to understand\nAs requirements change, only need to update code in one place instead of many\nEliminate chance of incremental mistakes when copy pasting\n\nOther advantages\n\nEasier to apply over groups or chunks in data, eg. by individual with lapply/map/data.table’s by\n\n\nWhen to write functions?\n\nWhen you’ve copy pasted more than twice\n\nHow to write functions?\n\nDevelop code for accomplishing the task\nAnalyse the code\n\nHow many inputs\nLook for duplication, eg. calculating the mean multiple times\n\nSave as intermediate value\n\n\nList inputs\nList arguments\nName function (verb)\n\nsnake_case\nCommon prefix eg input_select, input_checkbox, input_text\nDon’t override existing functions (see conflicted later)\n\nName arguments (nouns)\nPlace developed code in body of the function\nCheck results with eg. NAs, missing values, known results\n\nConditionals\n\nif else (else if)\n|| && combine multiple logical expressions and short-circuit when || sees the first true and && sees the first false\n| & are vectorized and cannot be directly used in an if unless you also use any / all\n\nChecking values\n\nstopifnot\nif else then stop\n\nReturn\n\nalways last thing\notherwise explicit return()\n\nEnvironment\n\nlexical scoping\n\nif variable isnt available in the functions environment, will look in environment where function was defined\nin a simple case, eg f(x) {x + y}, this means y, but also { and +\nif + is reassigned it will be used, meaning you can override a function from eg. base R\n\nhttps://adv-r.hadley.nz/environments.html\n\n\n\n\nNote: we’ll use targets’ dynamic and static branching instead for this workshop\nIterating over a single list/vector/etc\n\nfor loops\npurrr::map\n\nMultiple inputs\n\npurrr::map2\n\nLoop for side effects\n\npurrr::walk\n\nManaging errors\n\npurrr::safely\ntry\npurrr::possibly\npurrr::quietly\n\n\n\n\n\n\n\n\n\n\nThree components of a function\n\narguments (formals()): arguments used to control the function\nbody (body()): code inside the function\nenvironment (environment()): determins how function finds values associated with names\n\nimplicit based on where you defined the function\n\n\nFunctions are objects\nPrimitive functions\n\nonly found in the base package\neg. sum, [\nthey are the exceptions to above, for primitive functions the following return NULL formals(), body(), environment()\nprimitive functions indicated by typeof(f) is “builtin” or “special”\n\nAnonymous functions are used without first assigning them\ndo.call if you have arguments already in a list\nCombining function calls\n\nnesting f(g(x)) is concise, good for short sequences but hard to read (right to left, inside out)\nintermediate objects, requires naming each intermediate object, useful when independent objects are useful otherwise not\npipes let you run and function and then then next one and then the next one (…) in a chain\n\nLexical scoping\n\nname masking\n\n\n\n\nR is at its heart a functional language\nFunctional languages have:\n\nfirst-class functions that behave like any other data structure. you can assign to variables, store them in lists, pass them as arguments, etc\nfunctions that are pure. pure functions satisfy two properties - 1) outputs depend only on inputs where rerunning the function with the same inputs will yield the same results, and 2) functions have no side-effects like changing global variables, writing to disk, displaying to the screen\n\nFor R, functions are ideally either very pure, or very impure (plotting, saving, etc)\nFunctionals are functions that take a function as input and returns a vector as output\n\neg. lapply, apply, tapply, purrr::map\ncommonly used as an alternative to for loops\n(lots of great background)\n\nFunction factories\n\neg. power1 &lt;- function(exp) function(x) x ^ exp, square &lt;- power1(2)\n(beyond scope of workshop)\n\nFunction operators\n\n\n\nChoices\n\nif (condition) true_action\nif (condition) true_action else false_action\nalso else if\ncondition must be length 1\nswitch (https://adv-r.hadley.nz/control-flow.html#switch)\n\nmore compact than a bunch of if else if else\n\n\nLoops\n\nfor\n\ncaution 1:length if length == 0, instead use seq_along, seq.int, etc.\n\nwhile\nrepeat\n\n\n\n\n\n\n\n\n\n\nDefining a function\n\nR automatically returns whichever variable is on the last line of the body of the function\n\nComposing functions\n\ncompose functions by combining eg. two functions into a new function that uses both\nnest function calls by passing the output of one function directly as input to the next (also see pipes)\n\nTesting\n\nuse simple example data instead of your actual data where you might not know what the output value should be\n\nError handling\n\nadd errors and warnings to ensure the inputs and arguments are appropriate eg. the expected class\nerrors and warnings help inform a user about the function’s expectations\n\nDefining defaults\n\nR uses the position of arguments, typically only recommended with a smaller number of arguments\nwith a larger number of arguments, use named arguments to ensure arguments are correctly passed\nset a default value for an argument by passing it with = in the function definition\n\n\n\n\n\nmake decisions using logical comparisons\n\n&gt;, &lt;\n\nuse logical comparisons with if else to define conditional statements\n&& and ||\n\n\n\n\n\nvectorized functions\nvector recycling\nfor or apply?\n\n\n\n\nDetails on variable scoping\n\n\n\n\n\n\n\nif, else control flow in R as conditional statements\nifelse is vectorized\nany returns TRUE if there is at least one TRUE in the vector\nall returns TRUE if all are TRUE in the vector\n\n\n\n\nAlso see above Software Carpentry: Programming with R’s section on Functions (some similar content)\nDefensive programming\n\ndefensive programming can involve including check conditions and throwing errors when something is not as expected\nuse stopifnot() to detect where a conditional statement is FALSE and return an error\n\n\n\n\n\n\n\nWhy is formal testing worth the trouble?\n\nthe informal approach is load the function, experiment with it in console or scrap script, repeat\nan informal approach works fine in the moment, but when you return to the function in the future, say to add a new argument or test a new data type, you have no record of the tests you ran the first time\nbenefits to writing formal tests\n\nfewer bugs since you are explicit about how the code should work\nbetter code structure because if you find it hard to write tests for your functions, it might be that the design of your functions is not ideal\nwhen you discover a bug, write a test that replicates the bug - then making the test pass becomes the concrete goal in fixing the bug\nwhen functions are well covered by tests (robust), you can be confident making changes without accidentally breaking something\n\n\nIntroduction testthat\nTest mechanics and workflow\nExpectations\n\ntesting for\n\nequality\nerrors\nmatch\nlength\ns3 class, s4 class, type\n\n\n\n\n\n\n\n\nhttps://speakerdeck.com/jennybc/how-to-name-files\n\n\n\n\n\n\nCommunication with the user:\n\nfatal errors: stop()\nwarnings: warning()\ninformative output: message()\ninvisible returns (eg. plots): invisible()\n\n\n\n\nEfficient base R\n\nif() vs ifelse()\nis.na() and anyNA()"
  },
  {
    "objectID": "resources/02-functions.html#r-for-data-science",
    "href": "resources/02-functions.html#r-for-data-science",
    "title": "Functions",
    "section": "",
    "text": "Links:\n\n1st edition\n2nd edition\nGitHub repository\n\n\n\nWhy write functions?\n\n“Improving reach as a data scientist”\n“Automate common tasks”\nThree big advantages\n\nGive a function an evocative name that makes your code easier to understand\nAs requirements change, only need to update code in one place instead of many\nEliminate chance of incremental mistakes when copy pasting\n\nOther advantages\n\nEasier to apply over groups or chunks in data, eg. by individual with lapply/map/data.table’s by\n\n\nWhen to write functions?\n\nWhen you’ve copy pasted more than twice\n\nHow to write functions?\n\nDevelop code for accomplishing the task\nAnalyse the code\n\nHow many inputs\nLook for duplication, eg. calculating the mean multiple times\n\nSave as intermediate value\n\n\nList inputs\nList arguments\nName function (verb)\n\nsnake_case\nCommon prefix eg input_select, input_checkbox, input_text\nDon’t override existing functions (see conflicted later)\n\nName arguments (nouns)\nPlace developed code in body of the function\nCheck results with eg. NAs, missing values, known results\n\nConditionals\n\nif else (else if)\n|| && combine multiple logical expressions and short-circuit when || sees the first true and && sees the first false\n| & are vectorized and cannot be directly used in an if unless you also use any / all\n\nChecking values\n\nstopifnot\nif else then stop\n\nReturn\n\nalways last thing\notherwise explicit return()\n\nEnvironment\n\nlexical scoping\n\nif variable isnt available in the functions environment, will look in environment where function was defined\nin a simple case, eg f(x) {x + y}, this means y, but also { and +\nif + is reassigned it will be used, meaning you can override a function from eg. base R\n\nhttps://adv-r.hadley.nz/environments.html\n\n\n\n\nNote: we’ll use targets’ dynamic and static branching instead for this workshop\nIterating over a single list/vector/etc\n\nfor loops\npurrr::map\n\nMultiple inputs\n\npurrr::map2\n\nLoop for side effects\n\npurrr::walk\n\nManaging errors\n\npurrr::safely\ntry\npurrr::possibly\npurrr::quietly"
  },
  {
    "objectID": "resources/02-functions.html#advanced-r",
    "href": "resources/02-functions.html#advanced-r",
    "title": "Functions",
    "section": "",
    "text": "Three components of a function\n\narguments (formals()): arguments used to control the function\nbody (body()): code inside the function\nenvironment (environment()): determins how function finds values associated with names\n\nimplicit based on where you defined the function\n\n\nFunctions are objects\nPrimitive functions\n\nonly found in the base package\neg. sum, [\nthey are the exceptions to above, for primitive functions the following return NULL formals(), body(), environment()\nprimitive functions indicated by typeof(f) is “builtin” or “special”\n\nAnonymous functions are used without first assigning them\ndo.call if you have arguments already in a list\nCombining function calls\n\nnesting f(g(x)) is concise, good for short sequences but hard to read (right to left, inside out)\nintermediate objects, requires naming each intermediate object, useful when independent objects are useful otherwise not\npipes let you run and function and then then next one and then the next one (…) in a chain\n\nLexical scoping\n\nname masking\n\n\n\n\nR is at its heart a functional language\nFunctional languages have:\n\nfirst-class functions that behave like any other data structure. you can assign to variables, store them in lists, pass them as arguments, etc\nfunctions that are pure. pure functions satisfy two properties - 1) outputs depend only on inputs where rerunning the function with the same inputs will yield the same results, and 2) functions have no side-effects like changing global variables, writing to disk, displaying to the screen\n\nFor R, functions are ideally either very pure, or very impure (plotting, saving, etc)\nFunctionals are functions that take a function as input and returns a vector as output\n\neg. lapply, apply, tapply, purrr::map\ncommonly used as an alternative to for loops\n(lots of great background)\n\nFunction factories\n\neg. power1 &lt;- function(exp) function(x) x ^ exp, square &lt;- power1(2)\n(beyond scope of workshop)\n\nFunction operators\n\n\n\nChoices\n\nif (condition) true_action\nif (condition) true_action else false_action\nalso else if\ncondition must be length 1\nswitch (https://adv-r.hadley.nz/control-flow.html#switch)\n\nmore compact than a bunch of if else if else\n\n\nLoops\n\nfor\n\ncaution 1:length if length == 0, instead use seq_along, seq.int, etc.\n\nwhile\nrepeat"
  },
  {
    "objectID": "resources/02-functions.html#software-carpentry-programming-with-r",
    "href": "resources/02-functions.html#software-carpentry-programming-with-r",
    "title": "Functions",
    "section": "",
    "text": "Defining a function\n\nR automatically returns whichever variable is on the last line of the body of the function\n\nComposing functions\n\ncompose functions by combining eg. two functions into a new function that uses both\nnest function calls by passing the output of one function directly as input to the next (also see pipes)\n\nTesting\n\nuse simple example data instead of your actual data where you might not know what the output value should be\n\nError handling\n\nadd errors and warnings to ensure the inputs and arguments are appropriate eg. the expected class\nerrors and warnings help inform a user about the function’s expectations\n\nDefining defaults\n\nR uses the position of arguments, typically only recommended with a smaller number of arguments\nwith a larger number of arguments, use named arguments to ensure arguments are correctly passed\nset a default value for an argument by passing it with = in the function definition\n\n\n\n\n\nmake decisions using logical comparisons\n\n&gt;, &lt;\n\nuse logical comparisons with if else to define conditional statements\n&& and ||\n\n\n\n\n\nvectorized functions\nvector recycling\nfor or apply?\n\n\n\n\nDetails on variable scoping"
  },
  {
    "objectID": "resources/02-functions.html#software-carpentry-r-for-reproducible-scientific-analysis",
    "href": "resources/02-functions.html#software-carpentry-r-for-reproducible-scientific-analysis",
    "title": "Functions",
    "section": "",
    "text": "if, else control flow in R as conditional statements\nifelse is vectorized\nany returns TRUE if there is at least one TRUE in the vector\nall returns TRUE if all are TRUE in the vector\n\n\n\n\nAlso see above Software Carpentry: Programming with R’s section on Functions (some similar content)\nDefensive programming\n\ndefensive programming can involve including check conditions and throwing errors when something is not as expected\nuse stopifnot() to detect where a conditional statement is FALSE and return an error"
  },
  {
    "objectID": "resources/02-functions.html#r-packages",
    "href": "resources/02-functions.html#r-packages",
    "title": "Functions",
    "section": "",
    "text": "Why is formal testing worth the trouble?\n\nthe informal approach is load the function, experiment with it in console or scrap script, repeat\nan informal approach works fine in the moment, but when you return to the function in the future, say to add a new argument or test a new data type, you have no record of the tests you ran the first time\nbenefits to writing formal tests\n\nfewer bugs since you are explicit about how the code should work\nbetter code structure because if you find it hard to write tests for your functions, it might be that the design of your functions is not ideal\nwhen you discover a bug, write a test that replicates the bug - then making the test pass becomes the concrete goal in fixing the bug\nwhen functions are well covered by tests (robust), you can be confident making changes without accidentally breaking something\n\n\nIntroduction testthat\nTest mechanics and workflow\nExpectations\n\ntesting for\n\nequality\nerrors\nmatch\nlength\ns3 class, s4 class, type"
  },
  {
    "objectID": "resources/02-functions.html#what-they-forgot-to-teach-you",
    "href": "resources/02-functions.html#what-they-forgot-to-teach-you",
    "title": "Functions",
    "section": "",
    "text": "https://speakerdeck.com/jennybc/how-to-name-files"
  },
  {
    "objectID": "resources/02-functions.html#efficient-r",
    "href": "resources/02-functions.html#efficient-r",
    "title": "Functions",
    "section": "",
    "text": "Communication with the user:\n\nfatal errors: stop()\nwarnings: warning()\ninformative output: message()\ninvisible returns (eg. plots): invisible()\n\n\n\n\nEfficient base R\n\nif() vs ifelse()\nis.na() and anyNA()"
  },
  {
    "objectID": "resources/03-targets.html",
    "href": "resources/03-targets.html",
    "title": "{targets}",
    "section": "",
    "text": "The {targets} documentation and help manual should be the first places to look for details on {targets}. We’ll highlight some pages and links below relevant to the workshop, but be sure to spend time looking through these resources. This is a great example of an R package that has been very thoroughly and thoughtfully documented covering everything from beginner examples to the minutiae of how the package is designed and orchestrates workflows.\n\n\nThis help page is a great place to start when you are looking for help using {targets}. It details things to do before posting, and when you have decided to post a question, where to post it. The {targets} developer prefers using GitHub Discussion Groups for troubleshooting and discussions instead of GitHub Issues.\n\n\n{targets} has a very active discussion group with plenty of past solutions and discussions to search through when you are looking for help.\n\n\n\n\nThe debugging page is helpful for debugging {targets} workflows, especially when they get bigger and more complicated. Debugging in {targets} is different is a good place to start.\n\n\n\nSome select documentation pages from the {targets} documentation:\n\ntar_target(): targets!\ntar_make(): run the workflow\ntar_meta(): metadata on the workflow including errors, warnings, run times\ntar_read(): read a target\ntar_load(): load a target\ntar_visnetwork(): visualize the dependency graph between targets, functions, and objects\ntar_option_set(): sets options for when the {targets} workflow is run\n\nBonus:\n\nuse_targets(): setups a project to use {targets}\ntar_github_actions(): writes a GitHub Actions workflow to run the {targets} workflow on every push to GitHub\n\n\n\nThis vignette is an high-level summary of the features of {targets}.\n\n\n\n\nThe {targets} R package user manual\nThis is the most complete guide to using {targets}. It has info on most of the questions that you will have (although can be complicated - be patient with yourself!). It explains what a target is, best practices for functions, and why error messages can be less intuitive in your {targets} workflow among many other things.\n\n\n\n\n\nThis is an official half-day interactive tutorial on {targets}. It is accompanied by:\n\nCode repository: https://github.com/wlandau/targets-tutorial\nPosit Cloud workspace to try it out without any required downloads or installation: https://posit.cloud/content/1699460\nIntroductory slides: https://wlandau.github.io/targets-tutorial/#1\n\n\n\n\n\n\n\nThis 4 minute video walkthrough gives a short example of how {targets} can be used. It is accompanied by:\n\nUser Manual: A walkthrough to get started\nCode repository: https://github.com/wlandau/targets-four-minutes\nPosit Cloud workspace to try it out without any required downloads or installation: https://posit.cloud/content/3946303\n\n\n\n\n\n\n\n\n\n{tarchetypes} extends {targets} by providing functions to do complex tasks with shortcuts and simpler syntax, making your life easier. For example:\n\ntar_group_by: define groups in a data.frame to split subsequent targets by\ntar_render, tar_quarto: literate programming with {rmarkdown} or {quarto}\ntar_file_read(): tracks a local file and reads the contents\n\nSimilar to {targets}, {tarchetypes} has a very active Discussion Group with plenty of discussions to search through for solutions, tips and tricks.\n\nDocumentation: https://docs.ropensci.org/tarchetypes/\nDiscussion Group: https://github.com/ropensci/tarchetypes/discussions\n\n\n\n\n{stantargets} is a package extending {targets} for Bayesian data analysis with Stan through {cmdstanr}.\n\nDocumentation: https://docs.ropensci.org/stantargets/\nDiscussion Group: https://github.com/ropensci/stantargets/discussions\n\n\n\n\n{jagstargets} is a package extending {targets} for Bayesian data analysis with JAGS through {R2jags}.\n\nDocumentation: https://wlandau.github.io/jagstargets\nDiscussion Group: https://github.com/wlandau/jagstargets/discussions\n\n\n\n\n{gittargets} is a package extending {targets} for data version control.\n\nDocumentation: https://github.com/ropensci/gittargets\nDiscussion Group: https://github.com/ropensci/gittargets/discussions\n\n\n\n\n\nTarget factories are an advanced approach to making pipeline construction easier. Instead of writing many tar_target() calls, a user can replace them with a call to a “target factory”. tar_file_read() from {tarchetypes} is a simple example of this - one call to tar_file_read() creates targets tracking the local file for changes and another to read the local file in with the provided command.\n\n\n\n\n{tflow} is Miles McBain’s opinionated template for {targets} workflows.\n\nCode repository: An opinionated lightweight template for smooth targets flows."
  },
  {
    "objectID": "resources/03-targets.html#help",
    "href": "resources/03-targets.html#help",
    "title": "{targets}",
    "section": "",
    "text": "This help page is a great place to start when you are looking for help using {targets}. It details things to do before posting, and when you have decided to post a question, where to post it. The {targets} developer prefers using GitHub Discussion Groups for troubleshooting and discussions instead of GitHub Issues.\n\n\n{targets} has a very active discussion group with plenty of past solutions and discussions to search through when you are looking for help."
  },
  {
    "objectID": "resources/03-targets.html#debugging",
    "href": "resources/03-targets.html#debugging",
    "title": "{targets}",
    "section": "",
    "text": "The debugging page is helpful for debugging {targets} workflows, especially when they get bigger and more complicated. Debugging in {targets} is different is a good place to start."
  },
  {
    "objectID": "resources/03-targets.html#documentation",
    "href": "resources/03-targets.html#documentation",
    "title": "{targets}",
    "section": "",
    "text": "Some select documentation pages from the {targets} documentation:\n\ntar_target(): targets!\ntar_make(): run the workflow\ntar_meta(): metadata on the workflow including errors, warnings, run times\ntar_read(): read a target\ntar_load(): load a target\ntar_visnetwork(): visualize the dependency graph between targets, functions, and objects\ntar_option_set(): sets options for when the {targets} workflow is run\n\nBonus:\n\nuse_targets(): setups a project to use {targets}\ntar_github_actions(): writes a GitHub Actions workflow to run the {targets} workflow on every push to GitHub\n\n\n\nThis vignette is an high-level summary of the features of {targets}."
  },
  {
    "objectID": "resources/03-targets.html#user-manual",
    "href": "resources/03-targets.html#user-manual",
    "title": "{targets}",
    "section": "",
    "text": "The {targets} R package user manual\nThis is the most complete guide to using {targets}. It has info on most of the questions that you will have (although can be complicated - be patient with yourself!). It explains what a target is, best practices for functions, and why error messages can be less intuitive in your {targets} workflow among many other things."
  },
  {
    "objectID": "resources/03-targets.html#courses",
    "href": "resources/03-targets.html#courses",
    "title": "{targets}",
    "section": "",
    "text": "This is an official half-day interactive tutorial on {targets}. It is accompanied by:\n\nCode repository: https://github.com/wlandau/targets-tutorial\nPosit Cloud workspace to try it out without any required downloads or installation: https://posit.cloud/content/1699460\nIntroductory slides: https://wlandau.github.io/targets-tutorial/#1"
  },
  {
    "objectID": "resources/03-targets.html#videos",
    "href": "resources/03-targets.html#videos",
    "title": "{targets}",
    "section": "",
    "text": "This 4 minute video walkthrough gives a short example of how {targets} can be used. It is accompanied by:\n\nUser Manual: A walkthrough to get started\nCode repository: https://github.com/wlandau/targets-four-minutes\nPosit Cloud workspace to try it out without any required downloads or installation: https://posit.cloud/content/3946303"
  },
  {
    "objectID": "resources/03-targets.html#extensions",
    "href": "resources/03-targets.html#extensions",
    "title": "{targets}",
    "section": "",
    "text": "{tarchetypes} extends {targets} by providing functions to do complex tasks with shortcuts and simpler syntax, making your life easier. For example:\n\ntar_group_by: define groups in a data.frame to split subsequent targets by\ntar_render, tar_quarto: literate programming with {rmarkdown} or {quarto}\ntar_file_read(): tracks a local file and reads the contents\n\nSimilar to {targets}, {tarchetypes} has a very active Discussion Group with plenty of discussions to search through for solutions, tips and tricks.\n\nDocumentation: https://docs.ropensci.org/tarchetypes/\nDiscussion Group: https://github.com/ropensci/tarchetypes/discussions\n\n\n\n\n{stantargets} is a package extending {targets} for Bayesian data analysis with Stan through {cmdstanr}.\n\nDocumentation: https://docs.ropensci.org/stantargets/\nDiscussion Group: https://github.com/ropensci/stantargets/discussions\n\n\n\n\n{jagstargets} is a package extending {targets} for Bayesian data analysis with JAGS through {R2jags}.\n\nDocumentation: https://wlandau.github.io/jagstargets\nDiscussion Group: https://github.com/wlandau/jagstargets/discussions\n\n\n\n\n{gittargets} is a package extending {targets} for data version control.\n\nDocumentation: https://github.com/ropensci/gittargets\nDiscussion Group: https://github.com/ropensci/gittargets/discussions\n\n\n\n\n\nTarget factories are an advanced approach to making pipeline construction easier. Instead of writing many tar_target() calls, a user can replace them with a call to a “target factory”. tar_file_read() from {tarchetypes} is a simple example of this - one call to tar_file_read() creates targets tracking the local file for changes and another to read the local file in with the provided command."
  },
  {
    "objectID": "resources/03-targets.html#tflow",
    "href": "resources/03-targets.html#tflow",
    "title": "{targets}",
    "section": "",
    "text": "{tflow} is Miles McBain’s opinionated template for {targets} workflows.\n\nCode repository: An opinionated lightweight template for smooth targets flows."
  },
  {
    "objectID": "resources/04-renv.html",
    "href": "resources/04-renv.html",
    "title": "{renv}",
    "section": "",
    "text": "The {renv} documentation and vignettes have lots of information on {renv}. We’ll point out the most relevant pages and links, because {renv} can be used for a wide variety of uses on different systems (local machine, Docker, etc.).\n\n\n\n\nThis vignette overviews what {renv} is, and how to use it.\nIt walks us through the workflow of {renv}:\n\n\nCall renv::init() to initialize a new project-local environment with a private R library,\nWork in the project as normal, installing and removing new R packages as they are needed in the project,\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock),\nContinue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\n\nIt also outlines important caveats:\n\n{renv} only solves one small part of the reproducible problem: recording the version of R and R packages that are used in a project\n\nIt does not help if a package version becomes no longer available in the future.\nIt also does not help if a system dependency is missing on the computer attempting to recover an {renv} environment. The system dependency is external and needs to be installed for the project to run successfully.\n\n\n\nHow do you share your project that has an {renv} environment with other people? This vignette walks through the details of collaborating with other users when you have an isolated environment.\n\n\n\nFrequently asked questions about {renv}, a great place to start when troubleshooting {renv}.\n\n\n\n\n\n\nA video by {renv} author Kevin Ushey, outlining why {renv} is important and how to use it.\n\nCode repository: https://github.com/kevinushey/2020-rstudio-conf\n\n\n\n\nDemo on how to set up {renv} in existing projects with biology professor/statistician."
  },
  {
    "objectID": "resources/04-renv.html#vignettes",
    "href": "resources/04-renv.html#vignettes",
    "title": "{renv}",
    "section": "",
    "text": "This vignette overviews what {renv} is, and how to use it.\nIt walks us through the workflow of {renv}:\n\n\nCall renv::init() to initialize a new project-local environment with a private R library,\nWork in the project as normal, installing and removing new R packages as they are needed in the project,\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock),\nContinue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\n\nIt also outlines important caveats:\n\n{renv} only solves one small part of the reproducible problem: recording the version of R and R packages that are used in a project\n\nIt does not help if a package version becomes no longer available in the future.\nIt also does not help if a system dependency is missing on the computer attempting to recover an {renv} environment. The system dependency is external and needs to be installed for the project to run successfully.\n\n\n\nHow do you share your project that has an {renv} environment with other people? This vignette walks through the details of collaborating with other users when you have an isolated environment.\n\n\n\nFrequently asked questions about {renv}, a great place to start when troubleshooting {renv}."
  },
  {
    "objectID": "resources/04-renv.html#videos",
    "href": "resources/04-renv.html#videos",
    "title": "{renv}",
    "section": "",
    "text": "A video by {renv} author Kevin Ushey, outlining why {renv} is important and how to use it.\n\nCode repository: https://github.com/kevinushey/2020-rstudio-conf\n\n\n\n\nDemo on how to set up {renv} in existing projects with biology professor/statistician."
  },
  {
    "objectID": "resources/05-conflicted.html",
    "href": "resources/05-conflicted.html",
    "title": "{conflicted}",
    "section": "",
    "text": "{conflicted}\nThe {conflicted} package is a substantially simpler package than {targets} and {renv}. The {conflicted} documentation has everything we need, but also check out this {conflicted} blog post for some details about why {conflicted} was developed."
  },
  {
    "objectID": "slides/functions.html",
    "href": "slides/functions.html",
    "title": "Functions",
    "section": "",
    "text": "Name\nArguments\n\nnouns\noptions that modify how the function runs\n\nBody\nReturn"
  },
  {
    "objectID": "slides/functions.html#structure",
    "href": "slides/functions.html#structure",
    "title": "Functions",
    "section": "",
    "text": "Name\nArguments\n\nnouns\noptions that modify how the function runs\n\nBody\nReturn"
  },
  {
    "objectID": "slides/functions.html#structure-1",
    "href": "slides/functions.html#structure-1",
    "title": "Functions",
    "section": "Structure",
    "text": "Structure\nname &lt;- function(arguments) {\n  body\n\n  return()\n}"
  },
  {
    "objectID": "slides/functions.html#naming",
    "href": "slides/functions.html#naming",
    "title": "Functions",
    "section": "Naming",
    "text": "Naming\nsnake_case\nverb_noun\n\nplot_cars()\ncount_trees()\nmodel_elephants()"
  },
  {
    "objectID": "slides/functions.html#planning",
    "href": "slides/functions.html#planning",
    "title": "Functions",
    "section": "Planning",
    "text": "Planning\nDefine inputs, outputs\nFunctions should strive to do one thing, with no side effects\n\na common exemption is writing files or generating plots"
  },
  {
    "objectID": "slides/functions.html#checks",
    "href": "slides/functions.html#checks",
    "title": "Functions",
    "section": "Checks",
    "text": "Checks\nMake sure users provide appropriate inputs and arguments\nHandle errors with informative errors\n\nstopifnot()\nstop()\nwarning()\nmessage()"
  },
  {
    "objectID": "slides/functions.html#control-flow",
    "href": "slides/functions.html#control-flow",
    "title": "Functions",
    "section": "Control flow",
    "text": "Control flow\nManage choices, if something do this, else do this\n\nif()\nelse()\nifelse()"
  },
  {
    "objectID": "slides/functions.html#testingapproach",
    "href": "slides/functions.html#testingapproach",
    "title": "Functions",
    "section": "Testing/approach",
    "text": "Testing/approach\nSimple approach\n\ntesting scripts\ninteractive tests\n\nExtended approach\n\nfull testing suites with {testthat} or {tinytest}"
  },
  {
    "objectID": "slides/projects.html",
    "href": "slides/projects.html",
    "title": "Project Setup & Management",
    "section": "",
    "text": "When you are starting the analysis for a project, what are the pieces that make up your project?\n\nHow do we organize these into groups?"
  },
  {
    "objectID": "slides/projects.html#structure-group-discussion-15-mins",
    "href": "slides/projects.html#structure-group-discussion-15-mins",
    "title": "Project Setup & Management",
    "section": "",
    "text": "When you are starting the analysis for a project, what are the pieces that make up your project?\n\nHow do we organize these into groups?"
  },
  {
    "objectID": "slides/projects.html#readme-thinkpairshare-30-mins",
    "href": "slides/projects.html#readme-thinkpairshare-30-mins",
    "title": "Project Setup & Management",
    "section": "README: Think/Pair/Share (30 mins)",
    "text": "README: Think/Pair/Share (30 mins)\n\nREADMEs are a landing page to our projects\nWhat do you need know when you first interact with someone’s project?\n\nPerson 1: project creator / Person 2: project user\nCome up with a list of attributes, information, etc. that you would need to use this project (to be shared later)\nScenario A: Graduated lab member + new lab member\nScenario B: Dataset/code from published paper + professional accessing these"
  },
  {
    "objectID": "slides/projects.html#rstudio-project-setup-20-mins",
    "href": "slides/projects.html#rstudio-project-setup-20-mins",
    "title": "Project Setup & Management",
    "section": "RStudio Project Setup (20 mins)",
    "text": "RStudio Project Setup (20 mins)\n\nWe have the pieces of the project and info we need in the README: let’s set up an RStudio Project\nMany advantages (outlined in the “Project Management” file under “Resources” on the sidebar)."
  },
  {
    "objectID": "slides/projects.html#break-15-mins",
    "href": "slides/projects.html#break-15-mins",
    "title": "Project Setup & Management",
    "section": "BREAK (15 mins)",
    "text": "BREAK (15 mins)"
  }
]